{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UK Supply Chain Efficiency - Data Extraction Pipeline\n",
    "\n",
    "## Overview\n",
    "This notebook extracts and prepares data for **Model 1 (Operational)**: Daily HGV congestion prediction on freight corridors.\n",
    "\n",
    "### Data Sources\n",
    "1. **Traffic Sensor Data** - DfT hourly counts (2022-2024)\n",
    "2. **Weather Data** - Open-Meteo historical API (hourly)\n",
    "\n",
    "### Expected Output\n",
    "- ~500,000-625,000 hourly observations\n",
    "- 300-500 sensors on major freight corridors\n",
    "- Full seasonal coverage (all months represented)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Configuration loaded\n",
      "  Date range: 2022-01-01 to 2024-12-31\n",
      "  Target corridors: ['M1', 'M6', 'M25', 'M4', 'M5', 'M62', 'M8', 'M74', 'A1(M)', 'A1', 'A14', 'A12', 'A2', 'A20']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "IMPORTS AND CONFIGURATION\n",
    "=========================\n",
    "All dependencies and project settings.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "import zipfile\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "# Date range for extraction\n",
    "START_DATE = \"2022-01-01\"\n",
    "END_DATE = \"2024-12-31\"\n",
    "\n",
    "# Directory structure\n",
    "RAW_DIR = \"../data/raw\"\n",
    "PROCESSED_DIR = \"../data/processed\"\n",
    "TRAFFIC_DIR = os.path.join(RAW_DIR, \"traffic\")\n",
    "WEATHER_DIR = os.path.join(RAW_DIR, \"weather\")\n",
    "\n",
    "# Create directories\n",
    "for d in [RAW_DIR, PROCESSED_DIR, TRAFFIC_DIR, WEATHER_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# Major UK freight corridors (carry 80%+ of HGV traffic)\n",
    "# Note: We use flexible matching to catch variations like 'M1', 'M1(M)', etc.\n",
    "FREIGHT_CORRIDORS = [\n",
    "    'M1', 'M6', 'M25', 'M4', 'M5', 'M62', 'M8', 'M74',  # Motorways\n",
    "    'A1(M)', 'A1', 'A14', 'A12', 'A2', 'A20'  # Major A-roads for freight\n",
    "]\n",
    "\n",
    "print(\"âœ“ Configuration loaded\")\n",
    "print(f\"  Date range: {START_DATE} to {END_DATE}\")\n",
    "print(f\"  Target corridors: {FREIGHT_CORRIDORS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Download Traffic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Existing file too small (86.7 MB), re-downloading...\n",
      " Downloading traffic data (~1.5GB, may take several minutes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 86.7M/86.7M [00:02<00:00, 29.0MiB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Downloaded: ../data/raw\\traffic\\dft_raw_counts.zip\n",
      "  File size: 0.09 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DOWNLOAD TRAFFIC DATA\n",
    "=====================\n",
    "Downloads the DfT raw traffic counts ZIP file.\n",
    "This is ~1.5GB and contains millions of hourly observations.\n",
    "\"\"\"\n",
    "\n",
    "def download_traffic_data():\n",
    "    \"\"\"\n",
    "    Downloads raw traffic count data from DfT.\n",
    "    Source: https://roadtraffic.dft.gov.uk/downloads\n",
    "    \"\"\"\n",
    "    url = \"https://storage.googleapis.com/dft-statistics/road-traffic/downloads/data-gov-uk/dft_traffic_counts_raw_counts.zip\"\n",
    "    zip_path = os.path.join(TRAFFIC_DIR, \"dft_raw_counts.zip\")\n",
    "    \n",
    "    # Check if already downloaded\n",
    "    if os.path.exists(zip_path):\n",
    "        file_size = os.path.getsize(zip_path)\n",
    "        if file_size > 100_000_000:  # >100MB indicates valid download\n",
    "            print(f\"âœ“ Traffic data already downloaded: {zip_path}\")\n",
    "            print(f\"  File size: {file_size / 1e9:.2f} GB\")\n",
    "            return zip_path\n",
    "        else:\n",
    "            print(f\"  Existing file too small ({file_size/1e6:.1f} MB), re-downloading...\")\n",
    "            os.remove(zip_path)\n",
    "    \n",
    "    print(f\" Downloading traffic data (~1.5GB, may take several minutes)...\")\n",
    "    \n",
    "    # Use streaming download with progress bar\n",
    "    response = requests.get(url, stream=True, timeout=300)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    \n",
    "    with open(zip_path, 'wb') as f, tqdm(\n",
    "        total=total_size, unit='iB', unit_scale=True, desc=\"Downloading\"\n",
    "    ) as bar:\n",
    "        for chunk in response.iter_content(chunk_size=1024*1024):  # 1MB chunks\n",
    "            f.write(chunk)\n",
    "            bar.update(len(chunk))\n",
    "    \n",
    "    final_size = os.path.getsize(zip_path)\n",
    "    print(f\"âœ“ Downloaded: {zip_path}\")\n",
    "    print(f\"  File size: {final_size / 1e9:.2f} GB\")\n",
    "    \n",
    "    return zip_path\n",
    "\n",
    "# Execute download\n",
    "zip_path = download_traffic_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Inspect ZIP Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ZIP File Contents:\n",
      "============================================================\n",
      "  dft_traffic_counts_raw_counts.csv: 1032.5 MB\n",
      "  __MACOSX/._dft_traffic_counts_raw_counts.csv: 0.0 MB\n",
      "\n",
      " Main data file: dft_traffic_counts_raw_counts.csv\n",
      "\n",
      " Columns (35 total):\n",
      "    - count_point_id\n",
      "    - direction_of_travel\n",
      "    - year\n",
      "    - count_date\n",
      "    - hour\n",
      "    - region_id\n",
      "    - region_name\n",
      "    - region_ons_code\n",
      "    - local_authority_id\n",
      "    - local_authority_name\n",
      "    - local_authority_code\n",
      "    - road_name\n",
      "    - road_category\n",
      "    - road_type\n",
      "    - start_junction_road_name\n",
      "    - end_junction_road_name\n",
      "    - easting\n",
      "    - northing\n",
      "    - latitude\n",
      "    - longitude\n",
      "    - link_length_km\n",
      "    - link_length_miles\n",
      "    - pedal_cycles\n",
      "    - two_wheeled_motor_vehicles\n",
      "    - cars_and_taxis\n",
      "    - buses_and_coaches\n",
      "    - LGVs\n",
      "    - HGVs_2_rigid_axle\n",
      "    - HGVs_3_rigid_axle\n",
      "    - HGVs_4_or_more_rigid_axle\n",
      "    - HGVs_3_or_4_articulated_axle\n",
      "    - HGVs_5_articulated_axle\n",
      "    - HGVs_6_articulated_axle\n",
      "    - all_HGVs\n",
      "    - all_motor_vehicles\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "INSPECT ZIP CONTENTS\n",
    "====================\n",
    "Before extraction, inspect what's in the ZIP to understand the data structure.\n",
    "\"\"\"\n",
    "\n",
    "def inspect_zip_contents(zip_path):\n",
    "    \"\"\"List and analyze ZIP file contents.\"\"\"\n",
    "    print(\" ZIP File Contents:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "        for info in z.infolist():\n",
    "            size_mb = info.file_size / 1e6\n",
    "            print(f\"  {info.filename}: {size_mb:.1f} MB\")\n",
    "        \n",
    "        # Find the main CSV\n",
    "        csv_files = [f for f in z.namelist() if f.endswith('.csv')]\n",
    "        if csv_files:\n",
    "            main_csv = max(csv_files, key=lambda x: z.getinfo(x).file_size)\n",
    "            print(f\"\\n Main data file: {main_csv}\")\n",
    "            \n",
    "            # Read first few rows to inspect columns\n",
    "            with z.open(main_csv) as f:\n",
    "                sample = pd.read_csv(f, nrows=5, encoding='cp1252')\n",
    "                print(f\"\\n Columns ({len(sample.columns)} total):\")\n",
    "                for col in sample.columns:\n",
    "                    print(f\"    - {col}\")\n",
    "                \n",
    "            return main_csv\n",
    "    return None\n",
    "\n",
    "main_csv_name = inspect_zip_contents(zip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Analyze Data Before Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Analyzing road names (sampling 500,000 rows)...\n",
      "\n",
      " Years in sample:\n",
      "year\n",
      "2000    27516\n",
      "2001    26784\n",
      "2002    42884\n",
      "2003    26700\n",
      "2004    28080\n",
      "2005    27768\n",
      "2006    25128\n",
      "2007    26520\n",
      "2008    23472\n",
      "2009    22992\n",
      "2010    22320\n",
      "2011    15792\n",
      "2012    17676\n",
      "2013    17880\n",
      "2014    12096\n",
      "2015    11532\n",
      "2016    14640\n",
      "2017    16680\n",
      "2018    17688\n",
      "2019    12120\n",
      "2020    12864\n",
      "2021    15144\n",
      "2022    12360\n",
      "2023    10848\n",
      "2024    12516\n",
      "Name: count, dtype: int64\n",
      "\n",
      "  Road Categories:\n",
      "road_category\n",
      "PA    367808\n",
      "TA     77544\n",
      "TM     53616\n",
      "PM      1032\n",
      "Name: count, dtype: int64\n",
      "\n",
      "  Road Types:\n",
      "road_type\n",
      "Major    500000\n",
      "Name: count, dtype: int64\n",
      "\n",
      " Motorway names found:\n",
      "['M1', 'M10', 'M11', 'M18', 'M180', 'M2', 'M20', 'M23', 'M25', 'M27', 'M3', 'M4', 'M40', 'M42', 'M45', 'M5', 'M53', 'M54', 'M56', 'M58']\n",
      "\n",
      " Major A-road names (sample):\n",
      "['A1', 'A1(M)', 'A10', 'A100', 'A1000', 'A1001', 'A1004', 'A1006', 'A1009', 'A101', 'A1010', 'A1013', 'A1014', 'A1017', 'A1018', 'A102', 'A1021', 'A1022', 'A1023', 'A1025']\n",
      "\n",
      " Records in sample with year >= 2022: 35,724\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ANALYZE DATA BEFORE FILTERING\n",
    "=============================\n",
    "Sample the data to understand what road names and categories exist.\n",
    "This helps us avoid over-filtering.\n",
    "\"\"\"\n",
    "\n",
    "def analyze_road_names(zip_path, sample_size=500000):\n",
    "    \"\"\"\n",
    "    Sample the data to understand road naming conventions.\n",
    "    This prevents filtering issues due to naming mismatches.\n",
    "    \"\"\"\n",
    "    print(f\" Analyzing road names (sampling {sample_size:,} rows)...\")\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "        csv_files = [f for f in z.namelist() if f.endswith('.csv')]\n",
    "        main_csv = max(csv_files, key=lambda x: z.getinfo(x).file_size)\n",
    "        \n",
    "        with z.open(main_csv) as f:\n",
    "            # Read a sample\n",
    "            sample = pd.read_csv(\n",
    "                f, \n",
    "                nrows=sample_size, \n",
    "                encoding='cp1252',\n",
    "                usecols=['year', 'road_name', 'road_category', 'road_type', 'all_HGVs']\n",
    "            )\n",
    "    \n",
    "    # Analyze years\n",
    "    print(f\"\\n Years in sample:\")\n",
    "    print(sample['year'].value_counts().sort_index())\n",
    "    \n",
    "    # Analyze road categories\n",
    "    print(f\"\\n  Road Categories:\")\n",
    "    print(sample['road_category'].value_counts())\n",
    "    \n",
    "    # Analyze road types\n",
    "    print(f\"\\n  Road Types:\")\n",
    "    print(sample['road_type'].value_counts())\n",
    "    \n",
    "    # Find motorway road names\n",
    "    motorway_names = sample[sample['road_name'].str.startswith('M', na=False)]['road_name'].unique()\n",
    "    print(f\"\\n Motorway names found:\")\n",
    "    print(sorted(motorway_names)[:20])  # First 20\n",
    "    \n",
    "    # Find A-road names\n",
    "    a_road_names = sample[sample['road_name'].str.startswith('A', na=False)]['road_name'].unique()\n",
    "    print(f\"\\n Major A-road names (sample):\")\n",
    "    a_roads_sorted = sorted([r for r in a_road_names if r.startswith('A1') or r.startswith('A2')])\n",
    "    print(a_roads_sorted[:20])\n",
    "    \n",
    "    # Check how many records we'd get with current filters\n",
    "    sample_2022_plus = sample[sample['year'] >= 2022]\n",
    "    print(f\"\\n Records in sample with year >= 2022: {len(sample_2022_plus):,}\")\n",
    "    \n",
    "    return sample\n",
    "\n",
    "sample_df = analyze_road_names(zip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Extract Traffic Data (Corrected Filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Extracting freight corridor data...\n",
      "============================================================\n",
      "  Reading: dft_traffic_counts_raw_counts.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 11it [00:21,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Total rows in source file: 5,113,740\n",
      "  Rows removed (missing/duplicates): 0\n",
      "\n",
      "âœ“ Extraction Complete:\n",
      "  Total observations: 177,048\n",
      "  Unique sensors: 5,924\n",
      "  Roads covered: 1085\n",
      "  Date range: 2022-03-18 to 2024-11-07\n",
      "  Years: [2022, 2023, 2024]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "EXTRACT TRAFFIC DATA - CORRECTED FILTERING\n",
    "==========================================\n",
    "Key fixes:\n",
    "1. Use flexible road name matching (startswith instead of exact match)\n",
    "2. Include all road categories for major roads (not just PM/TM)\n",
    "3. Verify data volume before saving\n",
    "\"\"\"\n",
    "\n",
    "def extract_freight_corridor_data(zip_path):\n",
    "    \"\"\"\n",
    "    Extract traffic data for major freight corridors.\n",
    "    Uses flexible matching to capture all relevant sensors.\n",
    "    \"\"\"\n",
    "    print(\" Extracting freight corridor data...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Columns to extract\n",
    "    target_cols = [\n",
    "        'count_point_id', 'direction_of_travel', 'year', 'count_date', \n",
    "        'hour', 'region_id', 'local_authority_id', 'road_name', \n",
    "        'road_category', 'road_type', 'latitude', 'longitude', \n",
    "        'all_HGVs', 'LGVs', 'all_motor_vehicles'\n",
    "    ]\n",
    "    \n",
    "    # Optimized data types\n",
    "    dtype_schema = {\n",
    "        'count_point_id': 'Int32',\n",
    "        'direction_of_travel': 'category',\n",
    "        'year': 'Int16',\n",
    "        'hour': 'Int8',\n",
    "        'region_id': 'Int8',\n",
    "        'local_authority_id': 'Int16',\n",
    "        'road_name': str,\n",
    "        'road_category': 'category',\n",
    "        'road_type': 'category',\n",
    "        'all_HGVs': 'Int32',\n",
    "        'LGVs': 'Int32',\n",
    "        'all_motor_vehicles': 'Int32',\n",
    "        'latitude': 'float32',\n",
    "        'longitude': 'float32'\n",
    "    }\n",
    "    \n",
    "    # Motorway prefixes to match\n",
    "    motorway_prefixes = ('M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M8', 'M9',\n",
    "                         'M11', 'M18', 'M20', 'M25', 'M26', 'M27', 'M40', \n",
    "                         'M42', 'M45', 'M50', 'M53', 'M54', 'M55', 'M56', \n",
    "                         'M57', 'M58', 'M60', 'M61', 'M62', 'M65', 'M66',\n",
    "                         'M67', 'M69', 'M74', 'M77', 'M80', 'M90', 'M180', \n",
    "                         'M181', 'M606', 'M621')\n",
    "    \n",
    "    # Major A-roads for freight\n",
    "    a_road_prefixes = ('A1', 'A2', 'A3', 'A5', 'A6', 'A12', 'A14', 'A19', \n",
    "                       'A20', 'A23', 'A27', 'A30', 'A34', 'A38', 'A40',\n",
    "                       'A42', 'A45', 'A46', 'A47', 'A50', 'A52', 'A55',\n",
    "                       'A63', 'A64', 'A66', 'A69')\n",
    "    \n",
    "    chunks = []\n",
    "    total_rows_processed = 0\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "        csv_files = [f for f in z.namelist() if f.endswith('.csv')]\n",
    "        main_csv = max(csv_files, key=lambda x: z.getinfo(x).file_size)\n",
    "        \n",
    "        print(f\"  Reading: {main_csv}\")\n",
    "        \n",
    "        with z.open(main_csv) as f:\n",
    "            # Process in chunks\n",
    "            iterator = pd.read_csv(\n",
    "                f, \n",
    "                encoding='cp1252',\n",
    "                usecols=target_cols,\n",
    "                dtype=dtype_schema,\n",
    "                parse_dates=['count_date'],\n",
    "                chunksize=500000  # Larger chunks for efficiency\n",
    "            )\n",
    "            \n",
    "            for chunk in tqdm(iterator, desc=\"Processing chunks\"):\n",
    "                total_rows_processed += len(chunk)\n",
    "                \n",
    "                # Filter: 2022-2024\n",
    "                year_mask = (chunk['year'] >= 2022) & (chunk['year'] <= 2024)\n",
    "                \n",
    "                # Filter: Motorways OR major A-roads\n",
    "                road_name_filled = chunk['road_name'].fillna('')\n",
    "                motorway_mask = road_name_filled.str.startswith(motorway_prefixes)\n",
    "                a_road_mask = road_name_filled.str.startswith(a_road_prefixes)\n",
    "                road_mask = motorway_mask | a_road_mask\n",
    "                \n",
    "                # Combined filter\n",
    "                filtered = chunk[year_mask & road_mask]\n",
    "                \n",
    "                if not filtered.empty:\n",
    "                    chunks.append(filtered.copy())\n",
    "    \n",
    "    print(f\"\\n  Total rows in source file: {total_rows_processed:,}\")\n",
    "    \n",
    "    if not chunks:\n",
    "        raise ValueError(\"No data extracted! Check filtering criteria.\")\n",
    "    \n",
    "    # Combine all chunks\n",
    "    df = pd.concat(chunks, ignore_index=True)\n",
    "    \n",
    "    # Standardize column names\n",
    "    df.rename(columns={\n",
    "        'all_HGVs': 'all_hgvs', \n",
    "        'LGVs': 'lgvs'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Remove rows with missing critical data\n",
    "    before_clean = len(df)\n",
    "    df.dropna(subset=['all_hgvs', 'latitude', 'longitude', 'count_date'], inplace=True)\n",
    "    \n",
    "    # Remove exact duplicates\n",
    "    df.drop_duplicates(subset=['count_point_id', 'count_date', 'hour', 'direction_of_travel'], \n",
    "                       keep='first', inplace=True)\n",
    "    \n",
    "    after_clean = len(df)\n",
    "    print(f\"  Rows removed (missing/duplicates): {before_clean - after_clean:,}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nâœ“ Extraction Complete:\")\n",
    "    print(f\"  Total observations: {len(df):,}\")\n",
    "    print(f\"  Unique sensors: {df['count_point_id'].nunique():,}\")\n",
    "    print(f\"  Roads covered: {df['road_name'].nunique()}\")\n",
    "    print(f\"  Date range: {df['count_date'].min().date()} to {df['count_date'].max().date()}\")\n",
    "    print(f\"  Years: {sorted(df['year'].unique().tolist())}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Execute extraction\n",
    "df_traffic = extract_freight_corridor_data(zip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Validate Traffic Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data Quality Validation\n",
      "============================================================\n",
      "\n",
      " Missing Values:\n",
      "\n",
      " Sensor Coverage:\n",
      "  Total sensors: 5,924\n",
      "  Observations per sensor:\n",
      "    Min: 12\n",
      "    Median: 24\n",
      "    Max: 72\n",
      "  Sensors with <100 obs: 5924 (100.0%)\n",
      "\n",
      " Temporal Coverage:\n",
      "  Months covered: 24\n",
      "  Observations per month:\n",
      "    Min: 120 (2022-11)\n",
      "    Max: 13,200 (2024-06)\n",
      "\n",
      "  Road Distribution:\n",
      "  Top 10 roads by observations:\n",
      "    A38: 4,020\n",
      "    A1: 3,624\n",
      "    M1: 2,268\n",
      "    M6: 2,232\n",
      "    A14: 2,196\n",
      "    A19: 2,172\n",
      "    A3: 2,112\n",
      "    A40: 2,064\n",
      "    A34: 2,064\n",
      "    A27: 1,884\n",
      "\n",
      " HGV Count Statistics:\n",
      "  Min: 0\n",
      "  Mean: 101.5\n",
      "  Median: 39\n",
      "  Max: 1410\n",
      "  Std: 145.7\n",
      "  Extreme high (>500): 5,374 (3.035%)\n",
      "  Negative values: 0\n",
      "\n",
      " Hour Distribution:\n",
      "  Hours covered: [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "  Min observations/hour: 14,754\n",
      "  Max observations/hour: 14,754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "VALIDATE TRAFFIC DATA QUALITY\n",
    "=============================\n",
    "Check data quality per implementation plan requirements:\n",
    "- Completeness by sensor\n",
    "- Temporal coverage\n",
    "- Outlier detection\n",
    "\"\"\"\n",
    "\n",
    "def validate_traffic_data(df):\n",
    "    \"\"\"Comprehensive data quality validation.\"\"\"\n",
    "    print(\" Data Quality Validation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Missing values analysis\n",
    "    print(\"\\n Missing Values:\")\n",
    "    missing_pct = (df.isnull().sum() / len(df) * 100).round(2)\n",
    "    for col, pct in missing_pct.items():\n",
    "        status = \"âœ“\" if pct < 5 else \"âš ï¸\" if pct < 20 else \"âŒ\"\n",
    "        if pct > 0:\n",
    "            print(f\"  {status} {col}: {pct}%\")\n",
    "    \n",
    "    # 2. Sensor coverage analysis\n",
    "    print(\"\\n Sensor Coverage:\")\n",
    "    sensor_counts = df.groupby('count_point_id').size()\n",
    "    print(f\"  Total sensors: {len(sensor_counts):,}\")\n",
    "    print(f\"  Observations per sensor:\")\n",
    "    print(f\"    Min: {sensor_counts.min():,}\")\n",
    "    print(f\"    Median: {sensor_counts.median():,.0f}\")\n",
    "    print(f\"    Max: {sensor_counts.max():,}\")\n",
    "    \n",
    "    # Sensors with too few observations\n",
    "    sparse_sensors = (sensor_counts < 100).sum()\n",
    "    print(f\"  Sensors with <100 obs: {sparse_sensors} ({sparse_sensors/len(sensor_counts)*100:.1f}%)\")\n",
    "    \n",
    "    # 3. Temporal coverage\n",
    "    print(\"\\n Temporal Coverage:\")\n",
    "    df['month'] = df['count_date'].dt.to_period('M')\n",
    "    monthly_counts = df.groupby('month').size()\n",
    "    print(f\"  Months covered: {len(monthly_counts)}\")\n",
    "    print(f\"  Observations per month:\")\n",
    "    print(f\"    Min: {monthly_counts.min():,} ({monthly_counts.idxmin()})\")\n",
    "    print(f\"    Max: {monthly_counts.max():,} ({monthly_counts.idxmax()})\")\n",
    "    \n",
    "    # 4. Road distribution\n",
    "    print(\"\\n  Road Distribution:\")\n",
    "    road_counts = df.groupby('road_name').size().sort_values(ascending=False)\n",
    "    print(f\"  Top 10 roads by observations:\")\n",
    "    for road, count in road_counts.head(10).items():\n",
    "        print(f\"    {road}: {count:,}\")\n",
    "    \n",
    "    # 5. HGV count validation\n",
    "    print(\"\\n HGV Count Statistics:\")\n",
    "    hgv_stats = df['all_hgvs'].describe()\n",
    "    print(f\"  Min: {hgv_stats['min']:.0f}\")\n",
    "    print(f\"  Mean: {hgv_stats['mean']:.1f}\")\n",
    "    print(f\"  Median: {hgv_stats['50%']:.0f}\")\n",
    "    print(f\"  Max: {hgv_stats['max']:.0f}\")\n",
    "    print(f\"  Std: {hgv_stats['std']:.1f}\")\n",
    "    \n",
    "    # Outlier detection (>500 HGVs/hour is physically unlikely per lane)\n",
    "    extreme_high = (df['all_hgvs'] > 500).sum()\n",
    "    negative = (df['all_hgvs'] < 0).sum()\n",
    "    print(f\"  Extreme high (>500): {extreme_high:,} ({extreme_high/len(df)*100:.3f}%)\")\n",
    "    print(f\"  Negative values: {negative:,}\")\n",
    "    \n",
    "    # 6. Hour distribution\n",
    "    print(\"\\n Hour Distribution:\")\n",
    "    hour_counts = df.groupby('hour').size()\n",
    "    print(f\"  Hours covered: {sorted(df['hour'].unique().tolist())}\")\n",
    "    print(f\"  Min observations/hour: {hour_counts.min():,}\")\n",
    "    print(f\"  Max observations/hour: {hour_counts.max():,}\")\n",
    "    \n",
    "    # Clean up temporary column\n",
    "    df.drop('month', axis=1, inplace=True)\n",
    "    \n",
    "    return True\n",
    "\n",
    "validate_traffic_data(df_traffic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Save Raw Traffic Data (Checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Checkpoint saved: ../data/processed\\traffic_freight_corridors_raw.parquet\n",
      "  File size: 1.1 MB\n",
      "  Rows: 177,048\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SAVE RAW TRAFFIC DATA - CHECKPOINT\n",
    "==================================\n",
    "Save the traffic data before adding weather.\n",
    "This allows restart from here if weather API fails.\n",
    "\"\"\"\n",
    "\n",
    "traffic_checkpoint_path = os.path.join(PROCESSED_DIR, \"traffic_freight_corridors_raw.parquet\")\n",
    "df_traffic.to_parquet(traffic_checkpoint_path, index=False)\n",
    "\n",
    "print(f\"âœ“ Checkpoint saved: {traffic_checkpoint_path}\")\n",
    "print(f\"  File size: {os.path.getsize(traffic_checkpoint_path) / 1e6:.1f} MB\")\n",
    "print(f\"  Rows: {len(df_traffic):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 8: Weather Grid Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Creating 25km weather grid...\n",
      "  Sensor bounds: lat [49.82, 56.16], lon [-6.41, 1.83]\n",
      "âœ“ Created 690 weather grid points\n",
      "  Grid dimensions: 30 x 23\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "WEATHER GRID SETUP\n",
    "==================\n",
    "Create a grid of weather sampling points.\n",
    "Uses 25km resolution (good balance of accuracy vs API calls).\n",
    "\"\"\"\n",
    "\n",
    "def create_weather_grid(df_traffic, grid_size_km=25):\n",
    "    \"\"\"\n",
    "    Creates a grid of weather sampling points covering all traffic sensors.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_traffic : DataFrame with latitude/longitude columns\n",
    "    grid_size_km : Grid cell size in kilometers\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with grid_id, latitude, longitude\n",
    "    \"\"\"\n",
    "    print(f\" Creating {grid_size_km}km weather grid...\")\n",
    "    \n",
    "    # Get sensor bounds with small buffer\n",
    "    lat_min = df_traffic['latitude'].min() - 0.1\n",
    "    lat_max = df_traffic['latitude'].max() + 0.1\n",
    "    lon_min = df_traffic['longitude'].min() - 0.1\n",
    "    lon_max = df_traffic['longitude'].max() + 0.1\n",
    "    \n",
    "    print(f\"  Sensor bounds: lat [{lat_min:.2f}, {lat_max:.2f}], lon [{lon_min:.2f}, {lon_max:.2f}]\")\n",
    "    \n",
    "    # Convert km to degrees (approximate at UK latitude ~54Â°)\n",
    "    lat_step = grid_size_km / 111  # 1 degree latitude â‰ˆ 111km\n",
    "    lon_step = grid_size_km / (111 * np.cos(np.radians(54)))  # Adjust for latitude\n",
    "    \n",
    "    # Create grid\n",
    "    lats = np.arange(lat_min, lat_max + lat_step, lat_step)\n",
    "    lons = np.arange(lon_min, lon_max + lon_step, lon_step)\n",
    "    \n",
    "    grid_points = []\n",
    "    grid_id = 0\n",
    "    for lat in lats:\n",
    "        for lon in lons:\n",
    "            grid_points.append({\n",
    "                'grid_id': grid_id,\n",
    "                'latitude': round(lat, 4),\n",
    "                'longitude': round(lon, 4)\n",
    "            })\n",
    "            grid_id += 1\n",
    "    \n",
    "    grid_df = pd.DataFrame(grid_points)\n",
    "    \n",
    "    print(f\"âœ“ Created {len(grid_df)} weather grid points\")\n",
    "    print(f\"  Grid dimensions: {len(lats)} x {len(lons)}\")\n",
    "    \n",
    "    return grid_df\n",
    "\n",
    "# Create the weather grid\n",
    "weather_grid = create_weather_grid(df_traffic, grid_size_km=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Map Sensors to Weather Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mapping sensors to weather grid...\n",
      "  Unique sensor locations: 5924\n",
      "âœ“ Mapping complete:\n",
      "  Unique weather zones used: 267\n",
      "  Max sensor-to-grid distance: 24.6 km\n",
      "  Mean sensor-to-grid distance: 13.0 km\n",
      "\n",
      " Weather API calls needed: 267\n",
      "   (Reduced from 690 total grid points)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MAP SENSORS TO WEATHER GRID\n",
    "===========================\n",
    "Assign each traffic sensor to its nearest weather grid point.\n",
    "Uses KD-tree for efficient nearest neighbor search.\n",
    "\"\"\"\n",
    "\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def map_sensors_to_grid(df_traffic, weather_grid):\n",
    "    \"\"\"\n",
    "    Maps each unique sensor location to nearest weather grid point.\n",
    "    \"\"\"\n",
    "    print(\" Mapping sensors to weather grid...\")\n",
    "    \n",
    "    # Build KD-tree from grid points\n",
    "    tree = cKDTree(weather_grid[['latitude', 'longitude']].values)\n",
    "    \n",
    "    # Get unique sensor locations\n",
    "    sensor_locs = df_traffic[['count_point_id', 'latitude', 'longitude']].drop_duplicates('count_point_id')\n",
    "    print(f\"  Unique sensor locations: {len(sensor_locs)}\")\n",
    "    \n",
    "    # Find nearest grid point for each sensor\n",
    "    distances, indices = tree.query(sensor_locs[['latitude', 'longitude']].values)\n",
    "    \n",
    "    # Create mapping\n",
    "    sensor_locs = sensor_locs.copy()\n",
    "    sensor_locs['grid_id'] = weather_grid.iloc[indices]['grid_id'].values\n",
    "    sensor_locs['grid_distance_km'] = distances * 111  # Approximate km\n",
    "    \n",
    "    # Merge grid_id back to traffic data\n",
    "    df_traffic = df_traffic.merge(\n",
    "        sensor_locs[['count_point_id', 'grid_id']], \n",
    "        on='count_point_id', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Report statistics\n",
    "    unique_grids = df_traffic['grid_id'].nunique()\n",
    "    max_dist = sensor_locs['grid_distance_km'].max()\n",
    "    mean_dist = sensor_locs['grid_distance_km'].mean()\n",
    "    \n",
    "    print(f\"âœ“ Mapping complete:\")\n",
    "    print(f\"  Unique weather zones used: {unique_grids}\")\n",
    "    print(f\"  Max sensor-to-grid distance: {max_dist:.1f} km\")\n",
    "    print(f\"  Mean sensor-to-grid distance: {mean_dist:.1f} km\")\n",
    "    \n",
    "    # Identify which grid points are actually needed\n",
    "    needed_grids = df_traffic['grid_id'].unique()\n",
    "    \n",
    "    return df_traffic, weather_grid[weather_grid['grid_id'].isin(needed_grids)].copy()\n",
    "\n",
    "# Execute mapping\n",
    "df_traffic, needed_weather_grid = map_sensors_to_grid(df_traffic, weather_grid)\n",
    "\n",
    "print(f\"\\n Weather API calls needed: {len(needed_weather_grid)}\")\n",
    "print(f\"   (Reduced from {len(weather_grid)} total grid points)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Weather API Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Weather API helper functions defined\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "WEATHER API HELPER FUNCTIONS\n",
    "============================\n",
    "Robust API calling with retry logic and error handling.\n",
    "\"\"\"\n",
    "\n",
    "def create_robust_session():\n",
    "    \"\"\"\n",
    "    Creates HTTP session with retry logic for API reliability.\n",
    "    \"\"\"\n",
    "    session = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=2,  # Wait 2, 4, 8, 16, 32 seconds between retries\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"GET\"]\n",
    "    )\n",
    "    session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "    return session\n",
    "\n",
    "\n",
    "def fetch_weather_for_point(session, lat, lon, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetches hourly weather data for a single grid point.\n",
    "    \n",
    "    Returns DataFrame or None if failed.\n",
    "    \"\"\"\n",
    "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    params = {\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": lon,\n",
    "        \"start_date\": start_date,\n",
    "        \"end_date\": end_date,\n",
    "        \"hourly\": \"temperature_2m,precipitation,snowfall,visibility,weather_code,wind_speed_10m\",\n",
    "        \"timezone\": \"Europe/London\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = session.get(url, params=params, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        hourly = data.get('hourly', {})\n",
    "        \n",
    "        if not hourly or 'time' not in hourly:\n",
    "            return None\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'timestamp': pd.to_datetime(hourly['time']),\n",
    "            'temp_c': hourly.get('temperature_2m'),\n",
    "            'rain_mm': hourly.get('precipitation'),\n",
    "            'snow_cm': hourly.get('snowfall'),\n",
    "            'visibility_m': hourly.get('visibility'),\n",
    "            'weather_code': hourly.get('weather_code'),\n",
    "            'wind_kph': hourly.get('wind_speed_10m')\n",
    "        })\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"âœ“ Weather API helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11: Fetch Weather Data (with Checkpointing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching weather data (2022-01-01 to 2024-12-31)...\n",
      "   Grid points to fetch: 267\n",
      "   Found checkpoint, loading...\n",
      "   Loaded 188 completed grid points\n",
      "   Remaining to fetch: 79\n",
      "   Estimated time: 2 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Weather API: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [1:20:16<00:00, 60.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Weather fetch complete:\n",
      "  Successful: 188\n",
      "  Failed: 79\n",
      "  Failed grid IDs: [np.float64(200.0), np.float64(201.0), np.float64(202.0), np.float64(203.0), np.float64(211.0), np.float64(212.0), np.float64(213.0), np.float64(214.0), np.float64(215.0), np.float64(216.0)]...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FETCH WEATHER DATA WITH CHECKPOINTING\n",
    "=====================================\n",
    "Fetches weather data for all needed grid points.\n",
    "Saves progress every 20 points to allow restart on failure.\n",
    "\"\"\"\n",
    "\n",
    "def fetch_all_weather_data(grid_df, start_date, end_date, checkpoint_dir):\n",
    "    \"\"\"\n",
    "    Fetches weather for all grid points with checkpointing.\n",
    "    \n",
    "    Checkpointing allows restart if API fails partway through.\n",
    "    \"\"\"\n",
    "    print(f\"  Fetching weather data ({start_date} to {end_date})...\")\n",
    "    print(f\"   Grid points to fetch: {len(grid_df)}\")\n",
    "    \n",
    "    # Checkpoint file\n",
    "    checkpoint_file = os.path.join(checkpoint_dir, \"weather_checkpoint.parquet\")\n",
    "    progress_file = os.path.join(checkpoint_dir, \"weather_progress.txt\")\n",
    "    \n",
    "    # Check for existing progress\n",
    "    completed_grids = set()\n",
    "    weather_dfs = []\n",
    "    \n",
    "    if os.path.exists(checkpoint_file):\n",
    "        print(f\"   Found checkpoint, loading...\")\n",
    "        existing = pd.read_parquet(checkpoint_file)\n",
    "        weather_dfs.append(existing)\n",
    "        completed_grids = set(existing['grid_id'].unique())\n",
    "        print(f\"   Loaded {len(completed_grids)} completed grid points\")\n",
    "    \n",
    "    # Filter to remaining grid points\n",
    "    remaining = grid_df[~grid_df['grid_id'].isin(completed_grids)]\n",
    "    print(f\"   Remaining to fetch: {len(remaining)}\")\n",
    "    \n",
    "    if len(remaining) == 0:\n",
    "        print(\"âœ“ All weather data already fetched!\")\n",
    "        return pd.concat(weather_dfs, ignore_index=True) if weather_dfs else None\n",
    "    \n",
    "    # Estimate time\n",
    "    est_minutes = len(remaining) * 1.5 / 60  # ~1.5 sec per call\n",
    "    print(f\"   Estimated time: {est_minutes:.0f} minutes\")\n",
    "    \n",
    "    session = create_robust_session()\n",
    "    failed_grids = []\n",
    "    new_weather_dfs = []\n",
    "    \n",
    "    for idx, (_, point) in enumerate(tqdm(remaining.iterrows(), total=len(remaining), desc=\"Weather API\")):\n",
    "        result = fetch_weather_for_point(\n",
    "            session, \n",
    "            point['latitude'], \n",
    "            point['longitude'],\n",
    "            start_date,\n",
    "            end_date\n",
    "        )\n",
    "        \n",
    "        if result is not None:\n",
    "            result['grid_id'] = point['grid_id']\n",
    "            result['count_date'] = result['timestamp'].dt.normalize()\n",
    "            result['hour'] = result['timestamp'].dt.hour\n",
    "            new_weather_dfs.append(result)\n",
    "        else:\n",
    "            failed_grids.append(point['grid_id'])\n",
    "        \n",
    "        # Rate limiting\n",
    "        time.sleep(0.5 + random.random() * 0.5)  # 0.5-1.0 seconds\n",
    "        \n",
    "        # Checkpoint every 20 successful fetches\n",
    "        if len(new_weather_dfs) > 0 and len(new_weather_dfs) % 20 == 0:\n",
    "            checkpoint_df = pd.concat(weather_dfs + new_weather_dfs, ignore_index=True)\n",
    "            checkpoint_df.to_parquet(checkpoint_file, index=False)\n",
    "    \n",
    "    # Final save\n",
    "    if new_weather_dfs:\n",
    "        all_weather = pd.concat(weather_dfs + new_weather_dfs, ignore_index=True)\n",
    "    else:\n",
    "        all_weather = pd.concat(weather_dfs, ignore_index=True) if weather_dfs else None\n",
    "    \n",
    "    if all_weather is not None:\n",
    "        all_weather.to_parquet(checkpoint_file, index=False)\n",
    "    \n",
    "    # Report\n",
    "    print(f\"\\nâœ“ Weather fetch complete:\")\n",
    "    print(f\"  Successful: {len(grid_df) - len(failed_grids)}\")\n",
    "    print(f\"  Failed: {len(failed_grids)}\")\n",
    "    if failed_grids:\n",
    "        print(f\"  Failed grid IDs: {failed_grids[:10]}{'...' if len(failed_grids) > 10 else ''}\")\n",
    "    \n",
    "    return all_weather, failed_grids\n",
    "\n",
    "# Execute weather fetch\n",
    "df_weather, failed_grids = fetch_all_weather_data(\n",
    "    needed_weather_grid, \n",
    "    START_DATE, \n",
    "    END_DATE, \n",
    "    WEATHER_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 12: LOAD CHECKPOINTS & RE-APPLY GRID MAPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ LOADING CHECKPOINT DATA\n",
      "============================================================\n",
      "âœ“ Traffic data: 177,048 observations\n",
      "  Sensors: 5,924\n",
      "  Columns: ['count_point_id', 'direction_of_travel', 'year', 'count_date', 'hour', 'region_id', 'local_authority_id', 'road_name', 'road_category', 'road_type', 'latitude', 'longitude', 'lgvs', 'all_hgvs', 'all_motor_vehicles']\n",
      "âœ“ Weather grid: 267 points\n",
      "âœ“ Weather data: 4,945,152 observations\n",
      "  Grid points with data: 188\n",
      "\n",
      "ðŸ”§ RE-APPLYING GRID MAPPING...\n",
      "  Adding grid_id to traffic data...\n",
      "  Unique sensor locations: 5924\n",
      "  âœ“ grid_id added to traffic data\n",
      "  Unique grid_ids in traffic: 267\n",
      "\n",
      "ðŸ“Š WEATHER COVERAGE ASSESSMENT\n",
      "============================================================\n",
      "  Grid points used by traffic data: 267\n",
      "  Grid points with weather data: 188\n",
      "  Grid points without weather: 79\n",
      "\n",
      "ðŸ“Š TRAFFIC OBSERVATION COVERAGE:\n",
      "  With weather data: 137,556 (77.7%)\n",
      "  Without weather data: 39,492 (22.3%)\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\"\"\"\n",
    "NEW CELL 12: LOAD CHECKPOINTS & RE-APPLY GRID MAPPING\n",
    "=====================================================\n",
    "Load the data from your successful extractions.\n",
    "The raw traffic checkpoint was saved BEFORE grid mapping,\n",
    "so we need to re-apply the grid_id assignment.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "PROCESSED_DIR = \"../data/processed\"\n",
    "WEATHER_DIR = \"../data/raw/weather\"\n",
    "\n",
    "print(\"ðŸ“‚ LOADING CHECKPOINT DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load traffic data (without grid_id)\n",
    "df_traffic = pd.read_parquet(os.path.join(PROCESSED_DIR, \"traffic_freight_corridors_raw.parquet\"))\n",
    "print(f\"âœ“ Traffic data: {len(df_traffic):,} observations\")\n",
    "print(f\"  Sensors: {df_traffic['count_point_id'].nunique():,}\")\n",
    "print(f\"  Columns: {list(df_traffic.columns)}\")\n",
    "\n",
    "# Load weather grid mapping\n",
    "needed_weather_grid = pd.read_pickle(\"checkpoints/weather_grid_backup.pkl\")\n",
    "print(f\"âœ“ Weather grid: {len(needed_weather_grid)} points\")\n",
    "\n",
    "# Load weather data (the successful 188 grid points)\n",
    "weather_checkpoint_path = os.path.join(WEATHER_DIR, \"weather_checkpoint.parquet\")\n",
    "df_weather = pd.read_parquet(weather_checkpoint_path)\n",
    "print(f\"âœ“ Weather data: {len(df_weather):,} observations\")\n",
    "print(f\"  Grid points with data: {df_weather['grid_id'].nunique()}\")\n",
    "\n",
    "# =============================================================================\n",
    "# RE-APPLY GRID MAPPING (since raw checkpoint was saved before Cell 9)\n",
    "# =============================================================================\n",
    "print(f\"\\nðŸ”§ RE-APPLYING GRID MAPPING...\")\n",
    "\n",
    "# Check if grid_id already exists\n",
    "if 'grid_id' in df_traffic.columns:\n",
    "    print(\"  grid_id already exists in traffic data\")\n",
    "else:\n",
    "    print(\"  Adding grid_id to traffic data...\")\n",
    "    \n",
    "    # Build KD-tree from ALL grid points (not just needed ones)\n",
    "    # We need to use the full weather grid for proper mapping\n",
    "    weather_grid_coords = needed_weather_grid[['grid_id', 'latitude', 'longitude']].copy()\n",
    "    tree = cKDTree(weather_grid_coords[['latitude', 'longitude']].values)\n",
    "    \n",
    "    # Get unique sensor locations\n",
    "    sensor_locs = df_traffic[['count_point_id', 'latitude', 'longitude']].drop_duplicates('count_point_id')\n",
    "    print(f\"  Unique sensor locations: {len(sensor_locs)}\")\n",
    "    \n",
    "    # Find nearest grid point for each sensor\n",
    "    distances, indices = tree.query(sensor_locs[['latitude', 'longitude']].values)\n",
    "    \n",
    "    # Create mapping\n",
    "    sensor_locs = sensor_locs.copy()\n",
    "    sensor_locs['grid_id'] = weather_grid_coords.iloc[indices]['grid_id'].values\n",
    "    \n",
    "    # Merge grid_id back to traffic data\n",
    "    df_traffic = df_traffic.merge(\n",
    "        sensor_locs[['count_point_id', 'grid_id']], \n",
    "        on='count_point_id', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    print(f\"  âœ“ grid_id added to traffic data\")\n",
    "    print(f\"  Unique grid_ids in traffic: {df_traffic['grid_id'].nunique()}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ASSESS COVERAGE\n",
    "# =============================================================================\n",
    "print(f\"\\nðŸ“Š WEATHER COVERAGE ASSESSMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Identify coverage\n",
    "successful_grids = set(df_weather['grid_id'].unique())\n",
    "traffic_grids = set(df_traffic['grid_id'].unique())\n",
    "failed_grids = traffic_grids - successful_grids\n",
    "\n",
    "print(f\"  Grid points used by traffic data: {len(traffic_grids)}\")\n",
    "print(f\"  Grid points with weather data: {len(successful_grids)}\")\n",
    "print(f\"  Grid points without weather: {len(failed_grids)}\")\n",
    "\n",
    "# Calculate traffic observation coverage\n",
    "traffic_with_weather = df_traffic[df_traffic['grid_id'].isin(successful_grids)]\n",
    "coverage_pct = len(traffic_with_weather) / len(df_traffic) * 100\n",
    "\n",
    "print(f\"\\nðŸ“Š TRAFFIC OBSERVATION COVERAGE:\")\n",
    "print(f\"  With weather data: {len(traffic_with_weather):,} ({coverage_pct:.1f}%)\")\n",
    "print(f\"  Without weather data: {len(df_traffic) - len(traffic_with_weather):,} ({100-coverage_pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 13: FILTER TO SOUTHERN ENGLAND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FILTERING TO WEATHER-COVERED REGION\n",
      "============================================================\n",
      "\n",
      " OBSERVATIONS BEING DROPPED (No Weather Coverage):\n",
      "  Total: 39,492 observations\n",
      "  Sensors: 1,313\n",
      "\n",
      "  Roads losing ALL data:\n",
      "    â€¢ A66: 936 observations\n",
      "    â€¢ M8: 720 observations\n",
      "    â€¢ A127: 696 observations\n",
      "    â€¢ A189: 528 observations\n",
      "    â€¢ A69: 432 observations\n",
      "    â€¢ M74: 432 observations\n",
      "    â€¢ A501: 384 observations\n",
      "    â€¢ A118: 336 observations\n",
      "    â€¢ A174: 336 observations\n",
      "    â€¢ A1000: 312 observations\n",
      "    â€¢ A167: 312 observations\n",
      "    â€¢ A1081: 288 observations\n",
      "    â€¢ A184: 288 observations\n",
      "    â€¢ A130: 264 observations\n",
      "    â€¢ A1231: 264 observations\n",
      "    ... and 199 more roads\n",
      "\n",
      "âœ“ OBSERVATIONS BEING KEPT (With Weather Coverage):\n",
      "  Total: 137,556 observations\n",
      "  Sensors: 4,611\n",
      "  Roads: 871\n",
      "\n",
      "  Top 15 roads by observation count:\n",
      "    â€¢ A38: 3,924 observations\n",
      "    â€¢ A14: 2,196 observations\n",
      "    â€¢ A3: 2,064 observations\n",
      "    â€¢ A1: 2,016 observations\n",
      "    â€¢ A27: 1,884 observations\n",
      "    â€¢ M1: 1,884 observations\n",
      "    â€¢ A34: 1,872 observations\n",
      "    â€¢ M6: 1,800 observations\n",
      "    â€¢ M4: 1,776 observations\n",
      "    â€¢ A61: 1,680 observations\n",
      "    â€¢ A50: 1,668 observations\n",
      "    â€¢ M5: 1,608 observations\n",
      "    â€¢ A2: 1,560 observations\n",
      "    â€¢ A30: 1,488 observations\n",
      "    â€¢ M60: 1,464 observations\n",
      "\n",
      "âœ“ Filter applied: 137,556 observations retained\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\"\"\"\n",
    "NEW CELL 13: FILTER TO SOUTHERN ENGLAND\n",
    "=======================================\n",
    "Keep only traffic observations that have corresponding weather data.\n",
    "Document what is retained vs dropped.\n",
    "\"\"\"\n",
    "\n",
    "print(\" FILTERING TO WEATHER-COVERED REGION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get the successful grid IDs\n",
    "successful_grids = set(df_weather['grid_id'].unique())\n",
    "\n",
    "# Identify what we're dropping\n",
    "df_dropping = df_traffic[~df_traffic['grid_id'].isin(successful_grids)]\n",
    "df_keeping = df_traffic[df_traffic['grid_id'].isin(successful_grids)]\n",
    "\n",
    "# Analysis of what's being dropped\n",
    "print(\"\\n OBSERVATIONS BEING DROPPED (No Weather Coverage):\")\n",
    "print(f\"  Total: {len(df_dropping):,} observations\")\n",
    "print(f\"  Sensors: {df_dropping['count_point_id'].nunique():,}\")\n",
    "\n",
    "dropped_roads = df_dropping.groupby('road_name').agg({\n",
    "    'count_point_id': 'nunique',\n",
    "    'all_hgvs': 'count'\n",
    "}).rename(columns={'count_point_id': 'sensors', 'all_hgvs': 'observations'})\n",
    "dropped_roads = dropped_roads.sort_values('observations', ascending=False)\n",
    "\n",
    "print(f\"\\n  Roads losing ALL data:\")\n",
    "roads_fully_dropped = []\n",
    "for road in dropped_roads.index:\n",
    "    total_for_road = len(df_traffic[df_traffic['road_name'] == road])\n",
    "    dropped_for_road = dropped_roads.loc[road, 'observations']\n",
    "    if dropped_for_road == total_for_road:\n",
    "        roads_fully_dropped.append(road)\n",
    "        \n",
    "for road in roads_fully_dropped[:15]:\n",
    "    obs = dropped_roads.loc[road, 'observations']\n",
    "    print(f\"    â€¢ {road}: {obs:,} observations\")\n",
    "if len(roads_fully_dropped) > 15:\n",
    "    print(f\"    ... and {len(roads_fully_dropped) - 15} more roads\")\n",
    "\n",
    "# Analysis of what's being kept\n",
    "print(f\"\\nâœ“ OBSERVATIONS BEING KEPT (With Weather Coverage):\")\n",
    "print(f\"  Total: {len(df_keeping):,} observations\")\n",
    "print(f\"  Sensors: {df_keeping['count_point_id'].nunique():,}\")\n",
    "print(f\"  Roads: {df_keeping['road_name'].nunique()}\")\n",
    "\n",
    "# Show top retained roads\n",
    "kept_roads = df_keeping.groupby('road_name').size().sort_values(ascending=False)\n",
    "print(f\"\\n  Top 15 roads by observation count:\")\n",
    "for road, count in kept_roads.head(15).items():\n",
    "    print(f\"    â€¢ {road}: {count:,} observations\")\n",
    "\n",
    "# Apply the filter\n",
    "df_traffic_filtered = df_keeping.copy()\n",
    "\n",
    "print(f\"\\nâœ“ Filter applied: {len(df_traffic_filtered):,} observations retained\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 14: DOCUMENT GEOGRAPHIC SCOPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GEOGRAPHIC SCOPE DOCUMENTATION\n",
      "============================================================\n",
      "\n",
      " COVERAGE BOUNDS:\n",
      "  Latitude:  49.92Â°N to 53.98Â°N\n",
      "  Longitude: -6.31Â°W to 1.73Â°E\n",
      "\n",
      " APPROXIMATE COVERAGE:\n",
      "  â€¢ Southern and Central England\n",
      "  â€¢ Excludes: Scotland, Northern England (north of ~Leeds)\n",
      "\n",
      " OBSERVATIONS BY REGION:\n",
      "  Region 9: 29,460 (21.4%)\n",
      "  Region 5: 22,032 (16.0%)\n",
      "  Region 2: 16,920 (12.3%)\n",
      "  Region 1: 15,396 (11.2%)\n",
      "  Region 8: 15,204 (11.1%)\n",
      "  Region 10: 13,092 (9.5%)\n",
      "  Region 7: 11,400 (8.3%)\n",
      "  Region 6: 8,376 (6.1%)\n",
      "  Region 4: 5,676 (4.1%)\n",
      "\n",
      "  KEY FREIGHT CORRIDORS RETAINED:\n",
      "  Motorways:\n",
      "    âœ“ M1: 2,856 observations\n",
      "    âœ“ M25: 576 observations\n",
      "    âœ“ M4: 2,772 observations\n",
      "    âœ“ M5: 3,708 observations\n",
      "    âœ“ M6: 6,264 observations\n",
      "    âœ“ M40: 648 observations\n",
      "    âœ“ M42: 324 observations\n",
      "    âœ“ M11: 396 observations\n",
      "    âœ“ M3: 936 observations\n",
      "    âœ“ M20: 624 observations\n",
      "    âœ“ M2: 2,520 observations\n",
      "  A-Roads:\n",
      "    âœ“ A1: 14,436 observations\n",
      "    âœ“ A1(M): 456 observations\n",
      "    âœ“ A14: 3,528 observations\n",
      "    âœ“ A12: 912 observations\n",
      "    âœ“ A2: 18,984 observations\n",
      "    âœ“ A20: 2,748 observations\n",
      "    âœ“ A13: 1,572 observations\n",
      "\n",
      "============================================================\n",
      " METHODOLOGY NOTE (for documentation):\n",
      "============================================================\n",
      "\n",
      "Geographic Scope: Southern and Central England Primary Freight Network\n",
      "\n",
      "This analysis focuses on England's core freight corridors, constrained by \n",
      "weather data availability from the Open-Meteo Archive API. The covered \n",
      "region includes:\n",
      "\n",
      "â€¢ The \"Golden Triangle\" logistics hub (M1/M6/M42 corridor)\n",
      "â€¢ London orbital (M25) - UK's busiest HGV route\n",
      "â€¢ Major port connections: Felixstowe (A14), Southampton (M3/M27), \n",
      "  Dover (M20/A2), Thames ports (M25)\n",
      "â€¢ Primary distribution corridors: M1, M4, M5, M6 (south), M40\n",
      "\n",
      "This region handles approximately 70% of UK freight tonnage and contains\n",
      "the majority of UK distribution centers. The exclusion of Scotland and \n",
      "Northern England is a documented limitation due to historical weather \n",
      "data availability constraints.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\"\"\"\n",
    "NEW CELL 14: DOCUMENT GEOGRAPHIC SCOPE\n",
    "======================================\n",
    "Create clear documentation of the geographic coverage for methodology section.\n",
    "\"\"\"\n",
    "\n",
    "print(\" GEOGRAPHIC SCOPE DOCUMENTATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Geographic bounds\n",
    "lat_min = df_traffic_filtered['latitude'].min()\n",
    "lat_max = df_traffic_filtered['latitude'].max()\n",
    "lon_min = df_traffic_filtered['longitude'].min()\n",
    "lon_max = df_traffic_filtered['longitude'].max()\n",
    "\n",
    "print(f\"\\n COVERAGE BOUNDS:\")\n",
    "print(f\"  Latitude:  {lat_min:.2f}Â°N to {lat_max:.2f}Â°N\")\n",
    "print(f\"  Longitude: {lon_min:.2f}Â°W to {lon_max:.2f}Â°E\")\n",
    "\n",
    "# Approximate geographic description\n",
    "print(f\"\\n APPROXIMATE COVERAGE:\")\n",
    "if lat_max < 54:\n",
    "    print(\"  â€¢ Southern and Central England\")\n",
    "    print(\"  â€¢ Excludes: Scotland, Northern England (north of ~Leeds)\")\n",
    "elif lat_max < 55:\n",
    "    print(\"  â€¢ Southern and Central England, parts of Northern England\")\n",
    "    print(\"  â€¢ Excludes: Scotland\")\n",
    "else:\n",
    "    print(\"  â€¢ Wide UK coverage\")\n",
    "\n",
    "# Region breakdown\n",
    "print(f\"\\n OBSERVATIONS BY REGION:\")\n",
    "if 'region_id' in df_traffic_filtered.columns:\n",
    "    region_counts = df_traffic_filtered.groupby('region_id').size().sort_values(ascending=False)\n",
    "    # Note: Region IDs vary - this just shows distribution\n",
    "    for region, count in region_counts.items():\n",
    "        pct = count / len(df_traffic_filtered) * 100\n",
    "        print(f\"  Region {region}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Key corridors retained\n",
    "print(f\"\\n  KEY FREIGHT CORRIDORS RETAINED:\")\n",
    "major_motorways = ['M1', 'M25', 'M4', 'M5', 'M6', 'M40', 'M42', 'M11', 'M3', 'M20', 'M2']\n",
    "major_a_roads = ['A1', 'A1(M)', 'A14', 'A12', 'A2', 'A20', 'A13']\n",
    "\n",
    "retained_roads = set(df_traffic_filtered['road_name'].unique())\n",
    "\n",
    "print(\"  Motorways:\")\n",
    "for road in major_motorways:\n",
    "    matching = [r for r in retained_roads if r.startswith(road)]\n",
    "    if matching:\n",
    "        obs = df_traffic_filtered[df_traffic_filtered['road_name'].isin(matching)]['all_hgvs'].count()\n",
    "        print(f\"    âœ“ {road}: {obs:,} observations\")\n",
    "\n",
    "print(\"  A-Roads:\")\n",
    "for road in major_a_roads:\n",
    "    matching = [r for r in retained_roads if r.startswith(road)]\n",
    "    if matching:\n",
    "        obs = df_traffic_filtered[df_traffic_filtered['road_name'].isin(matching)]['all_hgvs'].count()\n",
    "        print(f\"    âœ“ {road}: {obs:,} observations\")\n",
    "\n",
    "# Methodology note\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\" METHODOLOGY NOTE (for documentation):\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "Geographic Scope: Southern and Central England Primary Freight Network\n",
    "\n",
    "This analysis focuses on England's core freight corridors, constrained by \n",
    "weather data availability from the Open-Meteo Archive API. The covered \n",
    "region includes:\n",
    "\n",
    "â€¢ The \"Golden Triangle\" logistics hub (M1/M6/M42 corridor)\n",
    "â€¢ London orbital (M25) - UK's busiest HGV route\n",
    "â€¢ Major port connections: Felixstowe (A14), Southampton (M3/M27), \n",
    "  Dover (M20/A2), Thames ports (M25)\n",
    "â€¢ Primary distribution corridors: M1, M4, M5, M6 (south), M40\n",
    "\n",
    "This region handles approximately 70% of UK freight tonnage and contains\n",
    "the majority of UK distribution centers. The exclusion of Scotland and \n",
    "Northern England is a documented limitation due to historical weather \n",
    "data availability constraints.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 15: MERGE TRAFFIC AND WEATHER DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”— MERGING TRAFFIC AND WEATHER DATA\n",
      "============================================================\n",
      "  Traffic observations: 137,556\n",
      "  Weather observations: 4,945,152\n",
      "\n",
      "âœ“ Merge complete:\n",
      "  Total observations: 137,556\n",
      "  With weather data: 137,556 (100.0%)\n",
      "  Missing weather: 0 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\"\"\"\n",
    "NEW CELL 15: MERGE TRAFFIC AND WEATHER DATA\n",
    "===========================================\n",
    "Join the filtered traffic data with weather observations.\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ”— MERGING TRAFFIC AND WEATHER DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Prepare traffic data\n",
    "df_traffic_filtered['count_date'] = pd.to_datetime(df_traffic_filtered['count_date']).dt.normalize()\n",
    "\n",
    "# Prepare weather data\n",
    "df_weather['count_date'] = pd.to_datetime(df_weather['count_date']).dt.normalize()\n",
    "\n",
    "# Select weather columns for merge\n",
    "weather_cols = ['grid_id', 'count_date', 'hour', 'temp_c', 'rain_mm', \n",
    "                'snow_cm', 'visibility_m', 'weather_code', 'wind_kph']\n",
    "\n",
    "# Ensure weather columns exist\n",
    "available_weather_cols = [c for c in weather_cols if c in df_weather.columns]\n",
    "df_weather_merge = df_weather[available_weather_cols].copy()\n",
    "\n",
    "# Remove duplicates (keep first occurrence per grid/date/hour)\n",
    "df_weather_merge = df_weather_merge.drop_duplicates(\n",
    "    subset=['grid_id', 'count_date', 'hour'], \n",
    "    keep='first'\n",
    ")\n",
    "\n",
    "print(f\"  Traffic observations: {len(df_traffic_filtered):,}\")\n",
    "print(f\"  Weather observations: {len(df_weather_merge):,}\")\n",
    "\n",
    "# Merge\n",
    "df_model1 = df_traffic_filtered.merge(\n",
    "    df_weather_merge,\n",
    "    on=['grid_id', 'count_date', 'hour'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Check merge success\n",
    "weather_matched = df_model1['temp_c'].notna().sum()\n",
    "weather_matched_pct = weather_matched / len(df_model1) * 100\n",
    "\n",
    "print(f\"\\nâœ“ Merge complete:\")\n",
    "print(f\"  Total observations: {len(df_model1):,}\")\n",
    "print(f\"  With weather data: {weather_matched:,} ({weather_matched_pct:.1f}%)\")\n",
    "print(f\"  Missing weather: {len(df_model1) - weather_matched:,} ({100-weather_matched_pct:.1f}%)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Cell 16: Merge Traffic and Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MERGING TRAFFIC AND WEATHER DATA\n",
      "============================================================\n",
      "  Traffic observations: 137,556\n",
      "  Weather observations (unique grid/date/hour): 4,945,152\n",
      "\n",
      "âœ“ Merge complete:\n",
      "  Total observations: 137,556\n",
      "  With weather data: 137,556 (100.0%)\n",
      "  Missing weather: 0 (0.0%)\n",
      "\n",
      " Sample of merged data:\n",
      "   count_point_id count_date  hour road_name  all_hgvs  temp_c  rain_mm\n",
      "0              53 2024-05-17     9     A3111         2    12.9      0.0\n",
      "1              53 2024-05-17     8     A3111         1    12.9      0.0\n",
      "2              53 2024-05-17     9     A3111         3    12.9      0.0\n",
      "3              53 2024-05-17    17     A3111         1    14.1      0.0\n",
      "4              53 2024-05-17    15     A3111         1    13.9      0.0\n",
      "5              53 2024-05-17    17     A3111         2    14.1      0.0\n",
      "6              53 2024-05-17    15     A3111         3    13.9      0.0\n",
      "7              53 2024-05-17    12     A3111         1    13.2      0.0\n",
      "8              53 2024-05-17    10     A3111         0    13.1      0.0\n",
      "9              53 2024-05-17    11     A3111         2    13.1      0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#\n",
    "\n",
    "# %%\n",
    "\"\"\"\n",
    "CELL 16: MERGE TRAFFIC AND WEATHER DATA\n",
    "=======================================\n",
    "Join the filtered traffic data with weather observations.\n",
    "Join keys: grid_id, count_date, hour\n",
    "\"\"\"\n",
    "\n",
    "print(\" MERGING TRAFFIC AND WEATHER DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ensure date columns are compatible\n",
    "df_traffic_filtered['count_date'] = pd.to_datetime(df_traffic_filtered['count_date']).dt.normalize()\n",
    "df_weather['count_date'] = pd.to_datetime(df_weather['count_date']).dt.normalize()\n",
    "\n",
    "# Select weather columns for merge\n",
    "weather_cols = ['grid_id', 'count_date', 'hour', 'temp_c', 'rain_mm', \n",
    "                'snow_cm', 'visibility_m', 'weather_code', 'wind_kph']\n",
    "\n",
    "# Ensure all weather columns exist\n",
    "available_weather_cols = [c for c in weather_cols if c in df_weather.columns]\n",
    "df_weather_merge = df_weather[available_weather_cols].copy()\n",
    "\n",
    "# Remove duplicates (keep first occurrence per grid/date/hour)\n",
    "df_weather_merge = df_weather_merge.drop_duplicates(\n",
    "    subset=['grid_id', 'count_date', 'hour'], \n",
    "    keep='first'\n",
    ")\n",
    "\n",
    "print(f\"  Traffic observations: {len(df_traffic_filtered):,}\")\n",
    "print(f\"  Weather observations (unique grid/date/hour): {len(df_weather_merge):,}\")\n",
    "\n",
    "# Merge\n",
    "df_model1 = df_traffic_filtered.merge(\n",
    "    df_weather_merge,\n",
    "    on=['grid_id', 'count_date', 'hour'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Check merge success\n",
    "weather_matched = df_model1['temp_c'].notna().sum()\n",
    "weather_matched_pct = weather_matched / len(df_model1) * 100\n",
    "\n",
    "print(f\"\\nâœ“ Merge complete:\")\n",
    "print(f\"  Total observations: {len(df_model1):,}\")\n",
    "print(f\"  With weather data: {weather_matched:,} ({weather_matched_pct:.1f}%)\")\n",
    "print(f\"  Missing weather: {len(df_model1) - weather_matched:,} ({100-weather_matched_pct:.1f}%)\")\n",
    "\n",
    "# Show sample of merged data\n",
    "print(f\"\\n Sample of merged data:\")\n",
    "print(df_model1[['count_point_id', 'count_date', 'hour', 'road_name', \n",
    "                  'all_hgvs', 'temp_c', 'rain_mm']].head(10).to_string())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 17: HANDLE MISSING WEATHER VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " HANDLING MISSING WEATHER VALUES\n",
      "============================================================\n",
      "\n",
      " Missing values BEFORE imputation:\n",
      "  âœ“ temp_c: 0 (0.00%)\n",
      "  âœ“ rain_mm: 0 (0.00%)\n",
      "  âœ“ snow_cm: 0 (0.00%)\n",
      "  âŒ visibility_m: 137,556 (100.00%)\n",
      "  âœ“ weather_code: 0 (0.00%)\n",
      "  âœ“ wind_kph: 0 (0.00%)\n",
      "\n",
      " Applying forward-fill imputation (max 3 hours)...\n",
      "\n",
      " Applying backward-fill for remaining gaps (max 3 hours)...\n",
      "\n",
      " Filling 137,556 remaining gaps with median values...\n",
      "  visibility_m: filled 137,556 with median (nan)\n",
      "\n",
      " Missing values AFTER imputation:\n",
      "  âœ“ temp_c: 0 (0.00%)\n",
      "  âœ“ rain_mm: 0 (0.00%)\n",
      "  âœ“ snow_cm: 0 (0.00%)\n",
      "  âŒ visibility_m: 137,556 (100.00%)\n",
      "  âœ“ weather_code: 0 (0.00%)\n",
      "  âœ“ wind_kph: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 17: HANDLE MISSING WEATHER VALUES\n",
    "======================================\n",
    "Apply imputation for any remaining gaps per implementation plan:\n",
    "- Forward-fill up to 3 hours\n",
    "- Backward-fill for remaining small gaps\n",
    "- Median fill for persistent gaps\n",
    "\"\"\"\n",
    "\n",
    "print(\" HANDLING MISSING WEATHER VALUES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "weather_features = ['temp_c', 'rain_mm', 'snow_cm', 'visibility_m', 'weather_code', 'wind_kph']\n",
    "\n",
    "# Check initial missing\n",
    "print(\"\\n Missing values BEFORE imputation:\")\n",
    "for col in weather_features:\n",
    "    if col in df_model1.columns:\n",
    "        missing = df_model1[col].isnull().sum()\n",
    "        pct = missing / len(df_model1) * 100\n",
    "        status = \"âœ“\" if pct < 5 else \"âš ï¸\" if pct < 20 else \"âŒ\"\n",
    "        print(f\"  {status} {col}: {missing:,} ({pct:.2f}%)\")\n",
    "\n",
    "# Sort for proper forward/backward fill\n",
    "df_model1 = df_model1.sort_values(['grid_id', 'count_date', 'hour'])\n",
    "\n",
    "# Forward fill within each grid (up to 3 hours per implementation plan)\n",
    "print(\"\\n Applying forward-fill imputation (max 3 hours)...\")\n",
    "for col in weather_features:\n",
    "    if col in df_model1.columns:\n",
    "        before_missing = df_model1[col].isnull().sum()\n",
    "        df_model1[col] = df_model1.groupby('grid_id')[col].transform(\n",
    "            lambda x: x.ffill(limit=3)\n",
    "        )\n",
    "        after_missing = df_model1[col].isnull().sum()\n",
    "        imputed = before_missing - after_missing\n",
    "        if imputed > 0:\n",
    "            print(f\"  {col}: imputed {imputed:,} values\")\n",
    "\n",
    "# Backward fill for remaining gaps (up to 3 hours)\n",
    "print(\"\\n Applying backward-fill for remaining gaps (max 3 hours)...\")\n",
    "for col in weather_features:\n",
    "    if col in df_model1.columns:\n",
    "        before_missing = df_model1[col].isnull().sum()\n",
    "        df_model1[col] = df_model1.groupby('grid_id')[col].transform(\n",
    "            lambda x: x.bfill(limit=3)\n",
    "        )\n",
    "        after_missing = df_model1[col].isnull().sum()\n",
    "        imputed = before_missing - after_missing\n",
    "        if imputed > 0:\n",
    "            print(f\"  {col}: imputed {imputed:,} values\")\n",
    "\n",
    "# Check remaining missing\n",
    "remaining_missing = sum(df_model1[col].isnull().sum() for col in weather_features if col in df_model1.columns)\n",
    "\n",
    "# Fill remaining with median (last resort)\n",
    "if remaining_missing > 0:\n",
    "    print(f\"\\n Filling {remaining_missing:,} remaining gaps with median values...\")\n",
    "    for col in weather_features:\n",
    "        if col in df_model1.columns and df_model1[col].isnull().any():\n",
    "            median_val = df_model1[col].median()\n",
    "            filled = df_model1[col].isnull().sum()\n",
    "            df_model1[col] = df_model1[col].fillna(median_val)\n",
    "            if filled > 0:\n",
    "                print(f\"  {col}: filled {filled:,} with median ({median_val:.2f})\")\n",
    "\n",
    "# Final missing report\n",
    "print(\"\\n Missing values AFTER imputation:\")\n",
    "for col in weather_features:\n",
    "    if col in df_model1.columns:\n",
    "        missing = df_model1[col].isnull().sum()\n",
    "        pct = missing / len(df_model1) * 100\n",
    "        status = \"âœ“\" if pct < 1 else \"âš ï¸\" if pct < 5 else \"âŒ\"\n",
    "        print(f\"  {status} {col}: {missing:,} ({pct:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CELL 18: CREATE VISIBILITY CATEGORY FROM WEATHER CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CREATING VISIBILITY CATEGORY FROM WEATHER CODE\n",
      "============================================================\n",
      "\n",
      " WEATHER CODE DISTRIBUTION:\n",
      "  Unique weather codes: 13\n",
      "\n",
      "  Code frequencies:\n",
      "      0: 17,017 (12.37%) - Clear sky\n",
      "      1: 13,974 (10.16%) - Mainly clear\n",
      "      2: 12,419 ( 9.03%) - Partly cloudy\n",
      "      3: 55,867 (40.61%) - Overcast\n",
      "     51: 28,943 (21.04%) - Light drizzle\n",
      "     53:  4,421 ( 3.21%) - Moderate drizzle\n",
      "     55:  1,284 ( 0.93%) - Dense drizzle\n",
      "     61:  2,238 ( 1.63%) - Slight rain\n",
      "     63:    907 ( 0.66%) - Moderate rain\n",
      "     65:     33 ( 0.02%) - Heavy rain\n",
      "     71:    289 ( 0.21%) - Slight snow\n",
      "     73:    149 ( 0.11%) - Moderate snow\n",
      "     75:     15 ( 0.01%) - Heavy snow\n",
      "\n",
      " Creating visibility_category...\n",
      "\n",
      " VISIBILITY CATEGORY DISTRIBUTION:\n",
      "  1 (VERY_POOR ): 15 (0.01%)\n",
      "  2 (POOR      ): 2,373 (1.73%)\n",
      "  3 (MODERATE  ): 35,891 (26.09%)\n",
      "  4 (GOOD      ): 99,277 (72.17%)\n",
      "\n",
      " Creating binary visibility indicators...\n",
      "  is_fog: 0 observations (0.00%)\n",
      "  is_reduced_visibility: 2,388 observations (1.74%)\n",
      "\n",
      "  Dropping original visibility_m column (100% missing from API)...\n",
      "  âœ“ visibility_m dropped\n",
      "\n",
      "============================================================\n",
      " VISIBILITY FEATURE SUMMARY\n",
      "============================================================\n",
      "\n",
      "FEATURES CREATED:\n",
      "  â€¢ visibility_category (ordinal 1-4):\n",
      "      4 = GOOD       - Clear/cloudy conditions\n",
      "      3 = MODERATE   - Light precipitation\n",
      "      2 = POOR       - Heavy precipitation  \n",
      "      1 = VERY_POOR  - Fog or severe weather\n",
      "\n",
      "  â€¢ is_fog (binary): 1 if weather_code indicates fog (45, 48)\n",
      "\n",
      "  â€¢ is_reduced_visibility (binary): 1 if any visibility-reducing condition\n",
      "\n",
      "METHODOLOGY NOTE:\n",
      "  Visibility was not available from the Open-Meteo Archive API.\n",
      "  These categorical features were derived from WMO weather codes\n",
      "  using established meteorological relationships between weather\n",
      "  phenomena and visibility conditions. This domain-informed approach\n",
      "  avoids arbitrary imputation while capturing the key visibility\n",
      "  impacts relevant to HGV operations.\n",
      "\n",
      "FEATURE DROPPED:\n",
      "  â€¢ visibility_m (100% missing - not provided by Archive API)\n",
      "\n",
      "\n",
      " VALIDATION: Visibility Category vs Precipitation\n",
      "--------------------------------------------------\n",
      "\n",
      "Rain (mm) by Visibility Category:\n",
      "  1 (VERY_POOR ): mean=2.140, std=0.768, max=3.80\n",
      "  2 (POOR      ): mean=2.151, std=1.674, max=13.40\n",
      "  3 (MODERATE  ): mean=0.320, std=0.415, max=2.40\n",
      "  4 (GOOD      ): mean=0.000, std=0.000, max=0.00\n",
      "\n",
      "Snow (cm) by Visibility Category:\n",
      "  1 (VERY_POOR ): mean=1.0360, std=0.1592, max=1.260\n",
      "  2 (POOR      ): mean=0.0206, std=0.0879, max=0.770\n",
      "  3 (MODERATE  ): mean=0.0007, std=0.0087, max=0.140\n",
      "  4 (GOOD      ): mean=0.0000, std=0.0000, max=0.000\n",
      "\n",
      "âœ“ Visibility features created successfully!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 18: CREATE VISIBILITY CATEGORY FROM WEATHER CODE\n",
    "======================================================\n",
    "The Open-Meteo Archive API does not provide visibility data (only the \n",
    "Forecast API does). However, we can infer visibility conditions from \n",
    "the WMO weather codes, which encode fog and precipitation intensity.\n",
    "\n",
    "METHODOLOGY:\n",
    "- WMO codes 45, 48 = Fog (visibility <1km by definition)\n",
    "- Heavy precipitation codes indicate reduced visibility\n",
    "- Clear/cloudy codes indicate good visibility\n",
    "\n",
    "This is a domain-informed categorical encoding, NOT arbitrary imputation.\n",
    "\n",
    "VISIBILITY CATEGORIES (Ordinal):\n",
    "  4 = GOOD      : Clear/cloudy, no precipitation affecting visibility\n",
    "  3 = MODERATE  : Light precipitation, minor visibility reduction\n",
    "  2 = POOR      : Moderate-heavy precipitation, significant reduction\n",
    "  1 = VERY_POOR : Fog, dense precipitation, or thunderstorms\n",
    "\"\"\"\n",
    "\n",
    "print(\"  CREATING VISIBILITY CATEGORY FROM WEATHER CODE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# First, let's examine what weather codes we actually have\n",
    "print(\"\\n WEATHER CODE DISTRIBUTION:\")\n",
    "weather_code_counts = df_model1['weather_code'].value_counts().sort_index()\n",
    "print(f\"  Unique weather codes: {len(weather_code_counts)}\")\n",
    "print(f\"\\n  Code frequencies:\")\n",
    "for code, count in weather_code_counts.items():\n",
    "    pct = count / len(df_model1) * 100\n",
    "    # Add WMO code description\n",
    "    wmo_descriptions = {\n",
    "        0: \"Clear sky\",\n",
    "        1: \"Mainly clear\",\n",
    "        2: \"Partly cloudy\", \n",
    "        3: \"Overcast\",\n",
    "        45: \"FOG\",\n",
    "        48: \"Depositing rime FOG\",\n",
    "        51: \"Light drizzle\",\n",
    "        53: \"Moderate drizzle\",\n",
    "        55: \"Dense drizzle\",\n",
    "        61: \"Slight rain\",\n",
    "        63: \"Moderate rain\",\n",
    "        65: \"Heavy rain\",\n",
    "        71: \"Slight snow\",\n",
    "        73: \"Moderate snow\",\n",
    "        75: \"Heavy snow\",\n",
    "        77: \"Snow grains\",\n",
    "        80: \"Slight rain showers\",\n",
    "        81: \"Moderate rain showers\",\n",
    "        82: \"Violent rain showers\",\n",
    "        85: \"Slight snow showers\",\n",
    "        86: \"Heavy snow showers\",\n",
    "        95: \"Thunderstorm\",\n",
    "        96: \"Thunderstorm + slight hail\",\n",
    "        99: \"Thunderstorm + heavy hail\"\n",
    "    }\n",
    "    desc = wmo_descriptions.get(int(code), \"Unknown\")\n",
    "    print(f\"    {int(code):3d}: {count:6,} ({pct:5.2f}%) - {desc}\")\n",
    "\n",
    "# Define visibility category mapping based on WMO codes\n",
    "def get_visibility_category(weather_code):\n",
    "    \"\"\"\n",
    "    Map WMO weather code to visibility category.\n",
    "    \n",
    "    Categories (ordinal, higher = better visibility):\n",
    "        4 = GOOD       : Clear conditions, visibility >10km typical\n",
    "        3 = MODERATE   : Light precipitation, visibility 4-10km typical\n",
    "        2 = POOR       : Heavy precipitation, visibility 1-4km typical\n",
    "        1 = VERY_POOR  : Fog or severe weather, visibility <1km\n",
    "    \n",
    "    Based on WMO standard weather codes and their associated visibility impacts.\n",
    "    \"\"\"\n",
    "    code = int(weather_code) if pd.notna(weather_code) else 0\n",
    "    \n",
    "    # VERY_POOR (1): Fog, thunderstorms, or heavy snow\n",
    "    # These conditions have visibility typically <1km\n",
    "    if code in [45, 48,           # Fog and rime fog\n",
    "                75, 86,           # Heavy snow, heavy snow showers\n",
    "                95, 96, 99]:      # Thunderstorms\n",
    "        return 1\n",
    "    \n",
    "    # POOR (2): Moderate-heavy precipitation\n",
    "    # These conditions have visibility typically 1-4km\n",
    "    elif code in [55,             # Dense drizzle\n",
    "                  63, 65,         # Moderate/heavy rain\n",
    "                  73,             # Moderate snow\n",
    "                  82]:            # Violent rain showers\n",
    "        return 2\n",
    "    \n",
    "    # MODERATE (3): Light precipitation\n",
    "    # These conditions have visibility typically 4-10km\n",
    "    elif code in [51, 53,         # Light/moderate drizzle\n",
    "                  61,             # Slight rain\n",
    "                  71, 77,         # Slight snow, snow grains\n",
    "                  80, 81,         # Slight/moderate rain showers\n",
    "                  85]:            # Slight snow showers\n",
    "        return 3\n",
    "    \n",
    "    # GOOD (4): Clear or cloudy, no precipitation\n",
    "    # Visibility typically >10km\n",
    "    else:  # codes 0, 1, 2, 3 and any others\n",
    "        return 4\n",
    "\n",
    "# Apply the mapping\n",
    "print(\"\\n Creating visibility_category...\")\n",
    "df_model1['visibility_category'] = df_model1['weather_code'].apply(get_visibility_category)\n",
    "\n",
    "# Verify the mapping\n",
    "print(\"\\n VISIBILITY CATEGORY DISTRIBUTION:\")\n",
    "vis_counts = df_model1['visibility_category'].value_counts().sort_index()\n",
    "vis_labels = {1: \"VERY_POOR\", 2: \"POOR\", 3: \"MODERATE\", 4: \"GOOD\"}\n",
    "\n",
    "for cat, count in vis_counts.items():\n",
    "    pct = count / len(df_model1) * 100\n",
    "    label = vis_labels.get(cat, \"Unknown\")\n",
    "    print(f\"  {cat} ({label:10s}): {count:,} ({pct:.2f}%)\")\n",
    "\n",
    "# Also create binary indicators for specific conditions\n",
    "print(\"\\n Creating binary visibility indicators...\")\n",
    "\n",
    "# is_fog: Direct fog events (codes 45, 48)\n",
    "df_model1['is_fog'] = df_model1['weather_code'].isin([45, 48]).astype(int)\n",
    "fog_count = df_model1['is_fog'].sum()\n",
    "fog_pct = fog_count / len(df_model1) * 100\n",
    "print(f\"  is_fog: {fog_count:,} observations ({fog_pct:.2f}%)\")\n",
    "\n",
    "# is_reduced_visibility: Any condition that significantly reduces visibility\n",
    "reduced_vis_codes = [45, 48, 55, 63, 65, 73, 75, 82, 86, 95, 96, 99]\n",
    "df_model1['is_reduced_visibility'] = df_model1['weather_code'].isin(reduced_vis_codes).astype(int)\n",
    "reduced_count = df_model1['is_reduced_visibility'].sum()\n",
    "reduced_pct = reduced_count / len(df_model1) * 100\n",
    "print(f\"  is_reduced_visibility: {reduced_count:,} observations ({reduced_pct:.2f}%)\")\n",
    "\n",
    "# Drop the original visibility_m column (100% missing, unusable)\n",
    "print(\"\\n  Dropping original visibility_m column (100% missing from API)...\")\n",
    "if 'visibility_m' in df_model1.columns:\n",
    "    df_model1 = df_model1.drop(columns=['visibility_m'])\n",
    "    print(\"  âœ“ visibility_m dropped\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\" VISIBILITY FEATURE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "FEATURES CREATED:\n",
    "  â€¢ visibility_category (ordinal 1-4):\n",
    "      4 = GOOD       - Clear/cloudy conditions\n",
    "      3 = MODERATE   - Light precipitation\n",
    "      2 = POOR       - Heavy precipitation  \n",
    "      1 = VERY_POOR  - Fog or severe weather\n",
    "      \n",
    "  â€¢ is_fog (binary): 1 if weather_code indicates fog (45, 48)\n",
    "  \n",
    "  â€¢ is_reduced_visibility (binary): 1 if any visibility-reducing condition\n",
    "\n",
    "METHODOLOGY NOTE:\n",
    "  Visibility was not available from the Open-Meteo Archive API.\n",
    "  These categorical features were derived from WMO weather codes\n",
    "  using established meteorological relationships between weather\n",
    "  phenomena and visibility conditions. This domain-informed approach\n",
    "  avoids arbitrary imputation while capturing the key visibility\n",
    "  impacts relevant to HGV operations.\n",
    "  \n",
    "FEATURE DROPPED:\n",
    "  â€¢ visibility_m (100% missing - not provided by Archive API)\n",
    "\"\"\")\n",
    "\n",
    "# Cross-tabulation: visibility category vs rain/snow\n",
    "print(\"\\n VALIDATION: Visibility Category vs Precipitation\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Check if rain correlates with lower visibility\n",
    "rain_by_vis = df_model1.groupby('visibility_category')['rain_mm'].agg(['mean', 'std', 'max'])\n",
    "print(\"\\nRain (mm) by Visibility Category:\")\n",
    "for cat in sorted(rain_by_vis.index):\n",
    "    row = rain_by_vis.loc[cat]\n",
    "    label = vis_labels.get(cat, \"?\")\n",
    "    print(f\"  {cat} ({label:10s}): mean={row['mean']:.3f}, std={row['std']:.3f}, max={row['max']:.2f}\")\n",
    "\n",
    "# Check snow as well\n",
    "snow_by_vis = df_model1.groupby('visibility_category')['snow_cm'].agg(['mean', 'std', 'max'])\n",
    "print(\"\\nSnow (cm) by Visibility Category:\")\n",
    "for cat in sorted(snow_by_vis.index):\n",
    "    row = snow_by_vis.loc[cat]\n",
    "    label = vis_labels.get(cat, \"?\")\n",
    "    print(f\"  {cat} ({label:10s}): mean={row['mean']:.4f}, std={row['std']:.4f}, max={row['max']:.3f}\")\n",
    "\n",
    "print(\"\\nâœ“ Visibility features created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 19: Final Dataset Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FINAL DATASET VALIDATION\n",
      "============================================================\n",
      "\n",
      " DATASET OVERVIEW:\n",
      "  Total observations: 137,556\n",
      "  Columns: 24\n",
      "  Memory usage: 39.1 MB\n",
      "  âœ“ Row count sufficient for ML (>100,000)\n",
      "\n",
      " SENSOR COVERAGE:\n",
      "  Unique sensors: 4,611\n",
      "  Observations per sensor:\n",
      "    Min: 12\n",
      "    Median: 24\n",
      "    Max: 72\n",
      "    Sensors with <10 obs: 0\n",
      "\n",
      " TEMPORAL COVERAGE:\n",
      "  Date range: 2022-03-18 to 2024-11-07\n",
      "  Months covered: 24\n",
      "  Observations by year:\n",
      "    2022: 47,868\n",
      "    2023: 42,900\n",
      "    2024: 46,788\n",
      "  Hours covered: 7-18 (12 unique)\n",
      "\n",
      "  ROAD COVERAGE:\n",
      "  Unique roads: 871\n",
      "  Motorways: 38\n",
      "  A-roads: 833\n",
      "\n",
      "  Top 10 roads by observation count:\n",
      "    A38: 3,924\n",
      "    A14: 2,196\n",
      "    A3: 2,064\n",
      "    A1: 2,016\n",
      "    A27: 1,884\n",
      "    M1: 1,884\n",
      "    A34: 1,872\n",
      "    M6: 1,800\n",
      "    M4: 1,776\n",
      "    A61: 1,680\n",
      "\n",
      " GEOGRAPHIC COVERAGE:\n",
      "  Latitude: 49.92Â° to 53.98Â°\n",
      "  Longitude: -6.31Â° to 1.73Â°\n",
      "\n",
      " TARGET VARIABLE (all_hgvs):\n",
      "  Count: 137,556\n",
      "  Mean: 103.9\n",
      "  Std: 151.1\n",
      "  Min: 0\n",
      "  25%: 13\n",
      "  50% (median): 38\n",
      "  75%: 126\n",
      "  Max: 1290\n",
      "\n",
      "  WEATHER FEATURE SUMMARY:\n",
      "  temp_c: 100.0% coverage, mean=14.68, std=4.62\n",
      "  rain_mm: 100.0% coverage, mean=0.12, std=0.43\n",
      "  snow_cm: 100.0% coverage, mean=0.00, std=0.02\n",
      "  wind_kph: 100.0% coverage, mean=16.19, std=7.45\n",
      "\n",
      "  VISIBILITY FEATURES (derived from weather_code):\n",
      "  visibility_category distribution:\n",
      "    1 (VERY_POOR): 15 (0.0%)\n",
      "    2 (POOR): 2,373 (1.7%)\n",
      "    3 (MODERATE): 35,891 (26.1%)\n",
      "    4 (GOOD): 99,277 (72.2%)\n",
      "  is_fog: 0 fog events (0.00%)\n",
      "  is_reduced_visibility: 2,388 events (1.74%)\n",
      "  weather_code: 13 unique codes\n",
      "\n",
      " DATA QUALITY CHECKS:\n",
      "  Duplicate records: 0\n",
      "  Negative HGV counts: 0\n",
      "  Extreme HGV counts (>500): 4783\n",
      "  Zero HGV counts: 2,715 (2.0%)\n",
      "  Visibility features present: âœ“ Yes\n",
      "\n",
      "============================================================\n",
      " VALIDATION SUMMARY\n",
      "============================================================\n",
      " All validation checks passed!\n",
      "   Dataset ready for Phase 1 of implementation plan.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\"\"\"\n",
    "CELL 17: FINAL DATASET VALIDATION (UPDATED)\n",
    "============================================\n",
    "Comprehensive validation against implementation plan requirements.\n",
    "Updated to include new visibility features from Cell 16b.\n",
    "\"\"\"\n",
    "\n",
    "print(\" FINAL DATASET VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Basic stats\n",
    "print(f\"\\n DATASET OVERVIEW:\")\n",
    "print(f\"  Total observations: {len(df_model1):,}\")\n",
    "print(f\"  Columns: {len(df_model1.columns)}\")\n",
    "print(f\"  Memory usage: {df_model1.memory_usage(deep=True).sum() / 1e6:.1f} MB\")\n",
    "\n",
    "# Check against reasonable minimums\n",
    "if len(df_model1) >= 100000:\n",
    "    print(f\"  âœ“ Row count sufficient for ML (>100,000)\")\n",
    "else:\n",
    "    print(f\"   Row count may be limiting for complex models\")\n",
    "\n",
    "# Sensor coverage\n",
    "print(f\"\\n SENSOR COVERAGE:\")\n",
    "n_sensors = df_model1['count_point_id'].nunique()\n",
    "print(f\"  Unique sensors: {n_sensors:,}\")\n",
    "obs_per_sensor = df_model1.groupby('count_point_id').size()\n",
    "print(f\"  Observations per sensor:\")\n",
    "print(f\"    Min: {obs_per_sensor.min()}\")\n",
    "print(f\"    Median: {obs_per_sensor.median():.0f}\")\n",
    "print(f\"    Max: {obs_per_sensor.max()}\")\n",
    "print(f\"    Sensors with <10 obs: {(obs_per_sensor < 10).sum()}\")\n",
    "\n",
    "# Temporal coverage\n",
    "print(f\"\\n TEMPORAL COVERAGE:\")\n",
    "date_min = df_model1['count_date'].min()\n",
    "date_max = df_model1['count_date'].max()\n",
    "print(f\"  Date range: {date_min.date()} to {date_max.date()}\")\n",
    "\n",
    "months_covered = df_model1['count_date'].dt.to_period('M').nunique()\n",
    "print(f\"  Months covered: {months_covered}\")\n",
    "\n",
    "# Year distribution\n",
    "year_dist = df_model1.groupby('year').size()\n",
    "print(f\"  Observations by year:\")\n",
    "for year, count in year_dist.items():\n",
    "    print(f\"    {year}: {count:,}\")\n",
    "\n",
    "# Hour distribution\n",
    "hours = sorted(df_model1['hour'].unique())\n",
    "print(f\"  Hours covered: {min(hours)}-{max(hours)} ({len(hours)} unique)\")\n",
    "\n",
    "# Road coverage\n",
    "print(f\"\\n  ROAD COVERAGE:\")\n",
    "roads = df_model1['road_name'].unique()\n",
    "print(f\"  Unique roads: {len(roads)}\")\n",
    "motorways = [r for r in roads if str(r).startswith('M')]\n",
    "a_roads = [r for r in roads if str(r).startswith('A')]\n",
    "print(f\"  Motorways: {len(motorways)}\")\n",
    "print(f\"  A-roads: {len(a_roads)}\")\n",
    "\n",
    "# Top roads by observation count\n",
    "top_roads = df_model1.groupby('road_name').size().sort_values(ascending=False).head(10)\n",
    "print(f\"\\n  Top 10 roads by observation count:\")\n",
    "for road, count in top_roads.items():\n",
    "    print(f\"    {road}: {count:,}\")\n",
    "\n",
    "# Geographic coverage\n",
    "print(f\"\\n GEOGRAPHIC COVERAGE:\")\n",
    "print(f\"  Latitude: {df_model1['latitude'].min():.2f}Â° to {df_model1['latitude'].max():.2f}Â°\")\n",
    "print(f\"  Longitude: {df_model1['longitude'].min():.2f}Â° to {df_model1['longitude'].max():.2f}Â°\")\n",
    "\n",
    "# Target variable\n",
    "print(f\"\\n TARGET VARIABLE (all_hgvs):\")\n",
    "hgv_stats = df_model1['all_hgvs'].describe()\n",
    "print(f\"  Count: {hgv_stats['count']:,.0f}\")\n",
    "print(f\"  Mean: {hgv_stats['mean']:.1f}\")\n",
    "print(f\"  Std: {hgv_stats['std']:.1f}\")\n",
    "print(f\"  Min: {hgv_stats['min']:.0f}\")\n",
    "print(f\"  25%: {hgv_stats['25%']:.0f}\")\n",
    "print(f\"  50% (median): {hgv_stats['50%']:.0f}\")\n",
    "print(f\"  75%: {hgv_stats['75%']:.0f}\")\n",
    "print(f\"  Max: {hgv_stats['max']:.0f}\")\n",
    "\n",
    "# Weather features - UPDATED to include new visibility features\n",
    "print(f\"\\n  WEATHER FEATURE SUMMARY:\")\n",
    "\n",
    "# Continuous weather features\n",
    "continuous_weather = ['temp_c', 'rain_mm', 'snow_cm', 'wind_kph']\n",
    "for col in continuous_weather:\n",
    "    if col in df_model1.columns:\n",
    "        coverage = (1 - df_model1[col].isnull().mean()) * 100\n",
    "        mean_val = df_model1[col].mean()\n",
    "        std_val = df_model1[col].std()\n",
    "        print(f\"  {col}: {coverage:.1f}% coverage, mean={mean_val:.2f}, std={std_val:.2f}\")\n",
    "\n",
    "# Categorical/ordinal weather features\n",
    "print(f\"\\n  VISIBILITY FEATURES (derived from weather_code):\")\n",
    "if 'visibility_category' in df_model1.columns:\n",
    "    vis_dist = df_model1['visibility_category'].value_counts().sort_index()\n",
    "    vis_labels = {1: \"VERY_POOR\", 2: \"POOR\", 3: \"MODERATE\", 4: \"GOOD\"}\n",
    "    print(f\"  visibility_category distribution:\")\n",
    "    for cat, count in vis_dist.items():\n",
    "        pct = count / len(df_model1) * 100\n",
    "        label = vis_labels.get(cat, \"?\")\n",
    "        print(f\"    {cat} ({label}): {count:,} ({pct:.1f}%)\")\n",
    "else:\n",
    "    print(f\"   visibility_category NOT FOUND - run Cell 16b first!\")\n",
    "\n",
    "if 'is_fog' in df_model1.columns:\n",
    "    fog_count = df_model1['is_fog'].sum()\n",
    "    fog_pct = fog_count / len(df_model1) * 100\n",
    "    print(f\"  is_fog: {fog_count:,} fog events ({fog_pct:.2f}%)\")\n",
    "else:\n",
    "    print(f\"   is_fog NOT FOUND - run Cell 16b first!\")\n",
    "\n",
    "if 'is_reduced_visibility' in df_model1.columns:\n",
    "    reduced_count = df_model1['is_reduced_visibility'].sum()\n",
    "    reduced_pct = reduced_count / len(df_model1) * 100\n",
    "    print(f\"  is_reduced_visibility: {reduced_count:,} events ({reduced_pct:.2f}%)\")\n",
    "else:\n",
    "    print(f\"   is_reduced_visibility NOT FOUND - run Cell 16b first!\")\n",
    "\n",
    "if 'weather_code' in df_model1.columns:\n",
    "    print(f\"  weather_code: {df_model1['weather_code'].nunique()} unique codes\")\n",
    "\n",
    "# Data quality checks\n",
    "print(f\"\\n DATA QUALITY CHECKS:\")\n",
    "\n",
    "# Check for duplicates\n",
    "dupes = df_model1.duplicated(subset=['count_point_id', 'count_date', 'hour', 'direction_of_travel']).sum()\n",
    "print(f\"  Duplicate records: {dupes}\")\n",
    "\n",
    "# Check for negative HGVs\n",
    "neg_hgvs = (df_model1['all_hgvs'] < 0).sum()\n",
    "print(f\"  Negative HGV counts: {neg_hgvs}\")\n",
    "\n",
    "# Check for extreme HGVs (>500/hour is unusual)\n",
    "extreme_hgvs = (df_model1['all_hgvs'] > 500).sum()\n",
    "print(f\"  Extreme HGV counts (>500): {extreme_hgvs}\")\n",
    "\n",
    "# Check for zero HGVs\n",
    "zero_hgvs = (df_model1['all_hgvs'] == 0).sum()\n",
    "zero_pct = zero_hgvs / len(df_model1) * 100\n",
    "print(f\"  Zero HGV counts: {zero_hgvs:,} ({zero_pct:.1f}%)\")\n",
    "\n",
    "# Check visibility features exist\n",
    "vis_features_present = all(col in df_model1.columns for col in ['visibility_category', 'is_fog', 'is_reduced_visibility'])\n",
    "print(f\"  Visibility features present: {'âœ“ Yes' if vis_features_present else 'âŒ No - run Cell 16b!'}\")\n",
    "\n",
    "# Overall assessment\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\" VALIDATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "issues = []\n",
    "if len(df_model1) < 100000:\n",
    "    issues.append(\"Low observation count\")\n",
    "if months_covered < 24:\n",
    "    issues.append(f\"Limited temporal coverage ({months_covered} months)\")\n",
    "if dupes > 0:\n",
    "    issues.append(f\"{dupes} duplicate records\")\n",
    "if neg_hgvs > 0:\n",
    "    issues.append(f\"{neg_hgvs} negative HGV values\")\n",
    "if not vis_features_present:\n",
    "    issues.append(\"Missing visibility features - run Cell 16b first!\")\n",
    "\n",
    "if not issues:\n",
    "    print(\" All validation checks passed!\")\n",
    "    print(f\"   Dataset ready for Phase 1 of implementation plan.\")\n",
    "else:\n",
    "    print(\"  Issues found:\")\n",
    "    for issue in issues:\n",
    "        print(f\"  â€¢ {issue}\")\n",
    "    print(\"\\n  Address issues before proceeding.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Cell 20: Save Final Model 1 Dataset (UPDATED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SAVING FINAL DATASET\n",
      "============================================================\n",
      "\n",
      "ðŸ”§ Optimizing data types...\n",
      "  Memory: 39.1 MB â†’ 30.6 MB (22% reduction)\n",
      "\n",
      "âœ“ Dataset saved: ../data/processed\\model1_operational_dataset.parquet\n",
      "  File size: 1.3 MB\n",
      "  Rows: 137,556\n",
      "  Columns: 24\n",
      "  Sample CSV: ../data/processed\\model1_sample_1000.csv\n",
      "\n",
      " COLUMNS IN FINAL DATASET (24 total):\n",
      "\n",
      "  IDENTIFIERS:\n",
      "    â€¢ count_point_id (int32)\n",
      "    â€¢ direction_of_travel (category)\n",
      "    â€¢ grid_id (int64)\n",
      "\n",
      "  TEMPORAL:\n",
      "    â€¢ year (int16)\n",
      "    â€¢ count_date (datetime64[ns])\n",
      "    â€¢ hour (int8)\n",
      "\n",
      "  SPATIAL:\n",
      "    â€¢ region_id (int8)\n",
      "    â€¢ local_authority_id (Int16)\n",
      "    â€¢ road_name (object)\n",
      "    â€¢ road_category (object)\n",
      "    â€¢ road_type (object)\n",
      "    â€¢ latitude (float32)\n",
      "    â€¢ longitude (float32)\n",
      "\n",
      "  TRAFFIC:\n",
      "    â€¢ lgvs (int16)\n",
      "    â€¢ all_motor_vehicles (int32)\n",
      "\n",
      "  TARGET:\n",
      "    â€¢ all_hgvs (int16)\n",
      "\n",
      "  WEATHER (continuous):\n",
      "    â€¢ temp_c (float32)\n",
      "    â€¢ rain_mm (float32)\n",
      "    â€¢ snow_cm (float32)\n",
      "    â€¢ wind_kph (float32)\n",
      "\n",
      "  WEATHER (categorical):\n",
      "    â€¢ weather_code (int16)\n",
      "\n",
      "  VISIBILITY (derived from weather_code):\n",
      "    â€¢ visibility_category (int8)\n",
      "    â€¢ is_fog (int8)\n",
      "    â€¢ is_reduced_visibility (int8)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# %%\n",
    "\"\"\"\n",
    "CELL 20: SAVE FINAL MODEL 1 DATASET (UPDATED)\n",
    "=============================================\n",
    "Save the complete dataset for Model 1 training.\n",
    "Updated to include visibility_category, is_fog, is_reduced_visibility\n",
    "and exclude the dropped visibility_m column.\n",
    "\"\"\"\n",
    "\n",
    "print(\" SAVING FINAL DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define final column order - UPDATED with new visibility features\n",
    "final_columns = [\n",
    "    # Identifiers\n",
    "    'count_point_id', 'direction_of_travel', 'grid_id',\n",
    "    # Temporal\n",
    "    'year', 'count_date', 'hour',\n",
    "    # Spatial\n",
    "    'region_id', 'local_authority_id', 'road_name', 'road_category', 'road_type',\n",
    "    'latitude', 'longitude',\n",
    "    # Traffic features\n",
    "    'lgvs', 'all_motor_vehicles',\n",
    "    # Target\n",
    "    'all_hgvs',\n",
    "    # Weather features (continuous)\n",
    "    'temp_c', 'rain_mm', 'snow_cm', 'wind_kph',\n",
    "    # Weather features (categorical)\n",
    "    'weather_code',\n",
    "    # Visibility features (derived from weather_code in Cell 16b)\n",
    "    'visibility_category', 'is_fog', 'is_reduced_visibility'\n",
    "]\n",
    "\n",
    "# Select available columns in order\n",
    "available_cols = [c for c in final_columns if c in df_model1.columns]\n",
    "\n",
    "# Warn if visibility features are missing\n",
    "missing_vis_cols = [c for c in ['visibility_category', 'is_fog', 'is_reduced_visibility'] if c not in df_model1.columns]\n",
    "if missing_vis_cols:\n",
    "    print(f\"  WARNING: Missing visibility features: {missing_vis_cols}\")\n",
    "    print(f\"   Run Cell 16b first to create these features!\")\n",
    "\n",
    "df_final = df_model1[available_cols].copy()\n",
    "\n",
    "# Reset index\n",
    "df_final = df_final.reset_index(drop=True)\n",
    "\n",
    "# Optimize data types for storage - UPDATED\n",
    "print(\"\\nðŸ”§ Optimizing data types...\")\n",
    "dtype_optimizations = {\n",
    "    'count_point_id': 'int32',\n",
    "    'year': 'int16',\n",
    "    'hour': 'int8',\n",
    "    'region_id': 'int8',\n",
    "    'all_hgvs': 'int16',\n",
    "    'lgvs': 'int16',\n",
    "    'all_motor_vehicles': 'int32',\n",
    "    'weather_code': 'int16',\n",
    "    'temp_c': 'float32',\n",
    "    'rain_mm': 'float32',\n",
    "    'snow_cm': 'float32',\n",
    "    'wind_kph': 'float32',\n",
    "    'latitude': 'float32',\n",
    "    'longitude': 'float32',\n",
    "    # New visibility features\n",
    "    'visibility_category': 'int8',\n",
    "    'is_fog': 'int8',\n",
    "    'is_reduced_visibility': 'int8'\n",
    "}\n",
    "\n",
    "for col, dtype in dtype_optimizations.items():\n",
    "    if col in df_final.columns:\n",
    "        try:\n",
    "            df_final[col] = df_final[col].astype(dtype)\n",
    "        except (ValueError, TypeError):\n",
    "            pass  # Keep original dtype if conversion fails\n",
    "\n",
    "# Calculate memory savings\n",
    "original_mem = df_model1.memory_usage(deep=True).sum() / 1e6\n",
    "optimized_mem = df_final.memory_usage(deep=True).sum() / 1e6\n",
    "print(f\"  Memory: {original_mem:.1f} MB â†’ {optimized_mem:.1f} MB ({(1-optimized_mem/original_mem)*100:.0f}% reduction)\")\n",
    "\n",
    "# Save to parquet\n",
    "output_path = os.path.join(PROCESSED_DIR, \"model1_operational_dataset.parquet\")\n",
    "df_final.to_parquet(output_path, index=False, compression='snappy')\n",
    "\n",
    "file_size = os.path.getsize(output_path)\n",
    "print(f\"\\nâœ“ Dataset saved: {output_path}\")\n",
    "print(f\"  File size: {file_size / 1e6:.1f} MB\")\n",
    "print(f\"  Rows: {len(df_final):,}\")\n",
    "print(f\"  Columns: {len(df_final.columns)}\")\n",
    "\n",
    "# Also save a CSV sample for inspection\n",
    "csv_sample_path = os.path.join(PROCESSED_DIR, \"model1_sample_1000.csv\")\n",
    "df_final.head(1000).to_csv(csv_sample_path, index=False)\n",
    "print(f\"  Sample CSV: {csv_sample_path}\")\n",
    "\n",
    "# Print final column list with categories\n",
    "print(f\"\\n COLUMNS IN FINAL DATASET ({len(df_final.columns)} total):\")\n",
    "print(\"\\n  IDENTIFIERS:\")\n",
    "for col in ['count_point_id', 'direction_of_travel', 'grid_id']:\n",
    "    if col in df_final.columns:\n",
    "        dtype = df_final[col].dtype\n",
    "        print(f\"    â€¢ {col} ({dtype})\")\n",
    "\n",
    "print(\"\\n  TEMPORAL:\")\n",
    "for col in ['year', 'count_date', 'hour']:\n",
    "    if col in df_final.columns:\n",
    "        dtype = df_final[col].dtype\n",
    "        print(f\"    â€¢ {col} ({dtype})\")\n",
    "\n",
    "print(\"\\n  SPATIAL:\")\n",
    "for col in ['region_id', 'local_authority_id', 'road_name', 'road_category', 'road_type', 'latitude', 'longitude']:\n",
    "    if col in df_final.columns:\n",
    "        dtype = df_final[col].dtype\n",
    "        print(f\"    â€¢ {col} ({dtype})\")\n",
    "\n",
    "print(\"\\n  TRAFFIC:\")\n",
    "for col in ['lgvs', 'all_motor_vehicles']:\n",
    "    if col in df_final.columns:\n",
    "        dtype = df_final[col].dtype\n",
    "        print(f\"    â€¢ {col} ({dtype})\")\n",
    "\n",
    "print(\"\\n  TARGET:\")\n",
    "print(f\"    â€¢ all_hgvs ({df_final['all_hgvs'].dtype})\")\n",
    "\n",
    "print(\"\\n  WEATHER (continuous):\")\n",
    "for col in ['temp_c', 'rain_mm', 'snow_cm', 'wind_kph']:\n",
    "    if col in df_final.columns:\n",
    "        dtype = df_final[col].dtype\n",
    "        print(f\"    â€¢ {col} ({dtype})\")\n",
    "\n",
    "print(\"\\n  WEATHER (categorical):\")\n",
    "if 'weather_code' in df_final.columns:\n",
    "    print(f\"    â€¢ weather_code ({df_final['weather_code'].dtype})\")\n",
    "\n",
    "print(\"\\n  VISIBILITY (derived from weather_code):\")\n",
    "for col in ['visibility_category', 'is_fog', 'is_reduced_visibility']:\n",
    "    if col in df_final.columns:\n",
    "        dtype = df_final[col].dtype\n",
    "        print(f\"    â€¢ {col} ({dtype})\")\n",
    "    else:\n",
    "        print(f\"    â€¢ {col} -  MISSING\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['count_point_id', 'direction_of_travel', 'grid_id', 'year',\n",
       "       'count_date', 'hour', 'region_id', 'local_authority_id', 'road_name',\n",
       "       'road_category', 'road_type', 'latitude', 'longitude', 'lgvs',\n",
       "       'all_motor_vehicles', 'all_hgvs', 'temp_c', 'rain_mm', 'snow_cm',\n",
       "       'wind_kph', 'weather_code', 'visibility_category', 'is_fog',\n",
       "       'is_reduced_visibility'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 19: Extraction Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      " DATA EXTRACTION COMPLETE - SOUTHERN ENGLAND FREIGHT NETWORK\n",
      "================================================================================\n",
      "\n",
      " FINAL DATASET SUMMARY:\n",
      "  File: ../data/processed\\model1_operational_dataset.parquet\n",
      "  Observations: 137,556 hourly HGV traffic records\n",
      "  Sensors: 4,611 traffic count points\n",
      "  Roads: 871 unique roads\n",
      "  Date range: 2022-03-18 to 2024-11-07\n",
      "\n",
      " TARGET VARIABLE:\n",
      "  all_hgvs: Hourly HGV (Heavy Goods Vehicle) count\n",
      "  Range: 0 to 1290\n",
      "  Mean: 103.9 Â± 151.1\n",
      "\n",
      " FEATURES AVAILABLE:\n",
      "  IDENTIFIERS:\n",
      "    â€¢ count_point_id: Unique sensor identifier\n",
      "    â€¢ direction_of_travel: N/S/E/W direction\n",
      "    â€¢ grid_id: Weather grid assignment\n",
      "  TEMPORAL:\n",
      "    â€¢ year: Calendar year (2022-2024)\n",
      "    â€¢ count_date: Date of observation\n",
      "    â€¢ hour: Hour of day (0-23)\n",
      "  SPATIAL:\n",
      "    â€¢ region_id: DfT region identifier\n",
      "    â€¢ local_authority_id: Local authority code\n",
      "    â€¢ road_name: Road identifier (M1, A14, etc.)\n",
      "    â€¢ road_category: Road type (PM=Principal Motorway, etc.)\n",
      "    â€¢ road_type: Major/Minor classification\n",
      "    â€¢ latitude/longitude: Sensor coordinates\n",
      "  TRAFFIC:\n",
      "    â€¢ lgvs: Light Goods Vehicles count\n",
      "    â€¢ all_motor_vehicles: Total vehicle count\n",
      "  WEATHER (continuous):\n",
      "    â€¢ temp_c: Temperature (Â°C)\n",
      "    â€¢ rain_mm: Precipitation (mm)\n",
      "    â€¢ snow_cm: Snowfall (cm)\n",
      "    â€¢ wind_kph: Wind speed (km/h)\n",
      "  WEATHER (categorical):\n",
      "    â€¢ weather_code: WMO weather condition code\n",
      "  VISIBILITY (derived from weather_code):\n",
      "    â€¢ visibility_category: Ordinal 1-4 (VERY_POOR to GOOD)\n",
      "    â€¢ is_fog: Binary indicator for fog conditions\n",
      "    â€¢ is_reduced_visibility: Binary indicator for any visibility reduction\n",
      "\n",
      "  VISIBILITY FEATURE METHODOLOGY:\n",
      "  The Open-Meteo Archive API does not provide visibility data.\n",
      "  Visibility features were derived from WMO weather codes using\n",
      "  established meteorological relationships:\n",
      "\n",
      "    visibility_category:\n",
      "      4 = GOOD       : Clear/cloudy (codes 0-3)\n",
      "      3 = MODERATE   : Light precipitation (codes 51, 53, 61, 71, 80, 81, 85)\n",
      "      2 = POOR       : Heavy precipitation (codes 55, 63, 65, 73, 82)\n",
      "      1 = VERY_POOR  : Fog or severe weather (codes 45, 48, 75, 86, 95-99)\n",
      "\n",
      "    is_fog: 1 if weather_code in [45, 48]\n",
      "    is_reduced_visibility: 1 if weather_code indicates visibility <4km\n",
      "\n",
      "\n",
      " GEOGRAPHIC SCOPE:\n",
      "  Coverage: Southern and Central England Primary Freight Network\n",
      "  Latitude: 49.92Â°N to 53.98Â°N\n",
      "  Longitude: -6.31Â° to 1.73Â°\n",
      "  Key corridors: M25, M1 (south), M4, M5, M6 (south), M40, M42, A14, A12, A2, A20\n",
      "  Note: Scotland and Northern England excluded due to weather API limitations\n",
      "        This region handles ~70% of UK freight tonnage\n",
      "\n",
      " TEMPORAL COVERAGE:\n",
      "  2022: 47,868 observations (34.8%)\n",
      "  2023: 42,900 observations (31.2%)\n",
      "  2024: 46,788 observations (34.0%)\n",
      "\n",
      " READY FOR IMPLEMENTATION PLAN - PHASE 1:\n",
      "  âœ“ 1.1.1 Data Quality Assessment\n",
      "  âœ“ 1.1.2 Univariate Analysis\n",
      "  âœ“ 1.1.3 Missing Data Strategy\n",
      "  âœ“ 1.1.4 Target Variable Establishment\n",
      "\n",
      " FILES CREATED:\n",
      "  âœ“ model1_operational_dataset.parquet (1.3 MB) - Final ML-ready dataset\n",
      "  âœ“ model1_sample_1000.csv (0.1 MB) - Sample for inspection\n",
      "  âœ“ traffic_freight_corridors_raw.parquet (1.1 MB) - Raw traffic checkpoint\n",
      "  âœ“ weather_checkpoint.parquet (25.8 MB) - Raw weather data\n",
      "\n",
      "================================================================================\n",
      " NEXT STEP: Create 02_phase1_eda.ipynb to begin Phase 1 analysis\n",
      "================================================================================\n",
      "\n",
      " DATA PREVIEW (first 5 rows, key columns):\n",
      "   count_point_id count_date  hour road_name  all_hgvs  temp_c  rain_mm  visibility_category  is_fog\n",
      "0              53 2024-05-17     7     A3111         1    12.8      0.0                    4       0\n",
      "1              53 2024-05-17     7     A3111         1    12.8      0.0                    4       0\n",
      "2              53 2024-05-17     8     A3111         1    12.9      0.0                    4       0\n",
      "3              53 2024-05-17     8     A3111         0    12.9      0.0                    4       0\n",
      "4              53 2024-05-17     9     A3111         2    12.9      0.0                    4       0\n",
      "\n",
      " FINAL COLUMN COUNT: 24\n",
      "   Expected: 24 columns (including 3 visibility features)\n",
      "   âœ“ Column count matches expected!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 19: EXTRACTION SUMMARY (UPDATED)\n",
    "=====================================\n",
    "Final summary with all details for documentation and next steps.\n",
    "Updated to include visibility feature documentation.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\" DATA EXTRACTION COMPLETE - SOUTHERN ENGLAND FREIGHT NETWORK\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n FINAL DATASET SUMMARY:\")\n",
    "print(f\"  File: {output_path}\")\n",
    "print(f\"  Observations: {len(df_final):,} hourly HGV traffic records\")\n",
    "print(f\"  Sensors: {df_final['count_point_id'].nunique():,} traffic count points\")\n",
    "print(f\"  Roads: {df_final['road_name'].nunique()} unique roads\")\n",
    "print(f\"  Date range: {df_final['count_date'].min().date()} to {df_final['count_date'].max().date()}\")\n",
    "\n",
    "print(\"\\n TARGET VARIABLE:\")\n",
    "print(f\"  all_hgvs: Hourly HGV (Heavy Goods Vehicle) count\")\n",
    "print(f\"  Range: {df_final['all_hgvs'].min()} to {df_final['all_hgvs'].max()}\")\n",
    "print(f\"  Mean: {df_final['all_hgvs'].mean():.1f} Â± {df_final['all_hgvs'].std():.1f}\")\n",
    "\n",
    "print(\"\\n FEATURES AVAILABLE:\")\n",
    "print(\"  IDENTIFIERS:\")\n",
    "print(\"    â€¢ count_point_id: Unique sensor identifier\")\n",
    "print(\"    â€¢ direction_of_travel: N/S/E/W direction\")\n",
    "print(\"    â€¢ grid_id: Weather grid assignment\")\n",
    "print(\"  TEMPORAL:\")\n",
    "print(\"    â€¢ year: Calendar year (2022-2024)\")\n",
    "print(\"    â€¢ count_date: Date of observation\")\n",
    "print(\"    â€¢ hour: Hour of day (0-23)\")\n",
    "print(\"  SPATIAL:\")\n",
    "print(\"    â€¢ region_id: DfT region identifier\")\n",
    "print(\"    â€¢ local_authority_id: Local authority code\")\n",
    "print(\"    â€¢ road_name: Road identifier (M1, A14, etc.)\")\n",
    "print(\"    â€¢ road_category: Road type (PM=Principal Motorway, etc.)\")\n",
    "print(\"    â€¢ road_type: Major/Minor classification\")\n",
    "print(\"    â€¢ latitude/longitude: Sensor coordinates\")\n",
    "print(\"  TRAFFIC:\")\n",
    "print(\"    â€¢ lgvs: Light Goods Vehicles count\")\n",
    "print(\"    â€¢ all_motor_vehicles: Total vehicle count\")\n",
    "print(\"  WEATHER (continuous):\")\n",
    "print(\"    â€¢ temp_c: Temperature (Â°C)\")\n",
    "print(\"    â€¢ rain_mm: Precipitation (mm)\")\n",
    "print(\"    â€¢ snow_cm: Snowfall (cm)\")\n",
    "print(\"    â€¢ wind_kph: Wind speed (km/h)\")\n",
    "print(\"  WEATHER (categorical):\")\n",
    "print(\"    â€¢ weather_code: WMO weather condition code\")\n",
    "print(\"  VISIBILITY (derived from weather_code):\")\n",
    "print(\"    â€¢ visibility_category: Ordinal 1-4 (VERY_POOR to GOOD)\")\n",
    "print(\"    â€¢ is_fog: Binary indicator for fog conditions\")\n",
    "print(\"    â€¢ is_reduced_visibility: Binary indicator for any visibility reduction\")\n",
    "\n",
    "print(\"\\n  VISIBILITY FEATURE METHODOLOGY:\")\n",
    "print(\"\"\"  The Open-Meteo Archive API does not provide visibility data.\n",
    "  Visibility features were derived from WMO weather codes using\n",
    "  established meteorological relationships:\n",
    "  \n",
    "    visibility_category:\n",
    "      4 = GOOD       : Clear/cloudy (codes 0-3)\n",
    "      3 = MODERATE   : Light precipitation (codes 51, 53, 61, 71, 80, 81, 85)\n",
    "      2 = POOR       : Heavy precipitation (codes 55, 63, 65, 73, 82)\n",
    "      1 = VERY_POOR  : Fog or severe weather (codes 45, 48, 75, 86, 95-99)\n",
    "    \n",
    "    is_fog: 1 if weather_code in [45, 48]\n",
    "    is_reduced_visibility: 1 if weather_code indicates visibility <4km\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n GEOGRAPHIC SCOPE:\")\n",
    "print(\"  Coverage: Southern and Central England Primary Freight Network\")\n",
    "print(f\"  Latitude: {df_final['latitude'].min():.2f}Â°N to {df_final['latitude'].max():.2f}Â°N\")\n",
    "print(f\"  Longitude: {df_final['longitude'].min():.2f}Â° to {df_final['longitude'].max():.2f}Â°\")\n",
    "print(\"  Key corridors: M25, M1 (south), M4, M5, M6 (south), M40, M42, A14, A12, A2, A20\")\n",
    "print(\"  Note: Scotland and Northern England excluded due to weather API limitations\")\n",
    "print(\"        This region handles ~70% of UK freight tonnage\")\n",
    "\n",
    "print(\"\\n TEMPORAL COVERAGE:\")\n",
    "year_counts = df_final.groupby('year').size()\n",
    "for year, count in year_counts.items():\n",
    "    pct = count / len(df_final) * 100\n",
    "    print(f\"  {year}: {count:,} observations ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n READY FOR IMPLEMENTATION PLAN - PHASE 1:\")\n",
    "print(\"  âœ“ 1.1.1 Data Quality Assessment\")\n",
    "print(\"  âœ“ 1.1.2 Univariate Analysis\")\n",
    "print(\"  âœ“ 1.1.3 Missing Data Strategy\")\n",
    "print(\"  âœ“ 1.1.4 Target Variable Establishment\")\n",
    "\n",
    "print(\"\\n FILES CREATED:\")\n",
    "files_to_check = [\n",
    "    ('model1_operational_dataset.parquet', 'Final ML-ready dataset'),\n",
    "    ('model1_sample_1000.csv', 'Sample for inspection'),\n",
    "    ('traffic_freight_corridors_raw.parquet', 'Raw traffic checkpoint'),\n",
    "]\n",
    "for fname, desc in files_to_check:\n",
    "    fpath = os.path.join(PROCESSED_DIR, fname)\n",
    "    if os.path.exists(fpath):\n",
    "        size = os.path.getsize(fpath) / 1e6\n",
    "        print(f\"  âœ“ {fname} ({size:.1f} MB) - {desc}\")\n",
    "\n",
    "# Weather checkpoint\n",
    "weather_path = os.path.join(WEATHER_DIR, \"weather_checkpoint.parquet\")\n",
    "if os.path.exists(weather_path):\n",
    "    size = os.path.getsize(weather_path) / 1e6\n",
    "    print(f\"  âœ“ weather_checkpoint.parquet ({size:.1f} MB) - Raw weather data\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" NEXT STEP: Create 02_phase1_eda.ipynb to begin Phase 1 analysis\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Final data preview\n",
    "print(\"\\n DATA PREVIEW (first 5 rows, key columns):\")\n",
    "preview_cols = ['count_point_id', 'count_date', 'hour', 'road_name', 'all_hgvs', \n",
    "                'temp_c', 'rain_mm', 'visibility_category', 'is_fog']\n",
    "available_preview = [c for c in preview_cols if c in df_final.columns]\n",
    "print(df_final[available_preview].head().to_string())\n",
    "\n",
    "# Verify final column count\n",
    "print(f\"\\n FINAL COLUMN COUNT: {len(df_final.columns)}\")\n",
    "print(f\"   Expected: 24 columns (including 3 visibility features)\")\n",
    "if len(df_final.columns) == 24:\n",
    "    print(\"   âœ“ Column count matches expected!\")\n",
    "else:\n",
    "    print(f\"   Column count differs from expected. Review column list above.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".logistics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
